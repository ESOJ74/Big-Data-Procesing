{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc157145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@65d0cb7\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .appName(\"Spark SQL KeepCoding Base\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d59cadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parquet: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parquet(spark: SparkSession): Unit = {\n",
    "    \n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"exercise2_output/*.json\")\n",
    "      .write\n",
    "      .format(\"parquet\")\n",
    "      .save(\"exercise3_output/spark-parquet\")\n",
    "    \n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"exercise2_output/*.json\")\n",
    "      .write\n",
    "      .partitionBy(\"curso\", \"clase\")\n",
    "      .format(\"parquet\")\n",
    "      .save(\"exercise3_output/spark-parquet-partitioned\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae070e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avro: (spark: org.apache.spark.sql.SparkSession)Unit\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  def avro(spark: SparkSession): Unit = {\n",
    "      \n",
    "    spark\n",
    "      .read\n",
    "      .format(\"json\")\n",
    "      .load(\"exercise2_output/*.json\")\n",
    "      .write\n",
    "      .option(\"avroSchema\", \"avro/highschool_user_schema_v1_0_0.avsc\")\n",
    "      .save(\"exercise3_output/spark-avro-user_schema_v1_0_0\")\n",
    "      \n",
    "    spark\n",
    "      .read\n",
    "      .option(\"avroSchema\", \"avro/highschool_user_schema_v1_1_0.avsc\")\n",
    "      .format(\"parquet\")\n",
    "      .load(\"exercise3_output/spark-avro-user_schema_v1_0_0/*.*\")\n",
    "      .withColumn(\"edad\", when($\"nombre\" === \"sara\" && $\"apellido\" === \"garcia\", 18L))\n",
    "      .write\n",
    "      .option(\"avroSchema\", \"avro/highschool_user_schema_v1_1_0.avsc\")\n",
    "      .format(\"json\")\n",
    "      .save(\"exercise3_output/spark-avro-user_schema_v1_1_0\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6adf53d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Path does not exist: file:/home/jose/ScalaSql/exercise3_output/spark-avro-user_schema_v1_0_0/*.*",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/jose/ScalaSql/exercise3_output/spark-avro-user_schema_v1_0_0/*.*",
      "  at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$3(DataSource.scala:792)",
      "  at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:372)",
      "  at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)",
      "  at scala.util.Success.$anonfun$map$1(Try.scala:255)",
      "  at scala.util.Success.map(Try.scala:213)",
      "  at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)",
      "  at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)",
      "  at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)",
      "  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)",
      "  at java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1402)",
      "  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)",
      "  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)",
      "  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)",
      "  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)",
      ""
     ]
    }
   ],
   "source": [
    "parquet(spark)\n",
    "avro(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237e756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8357b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

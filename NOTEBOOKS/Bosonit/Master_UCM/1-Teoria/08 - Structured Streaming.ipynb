{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming: convirtiendo consultas batch en streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p><b>PARA SABER MÁS</b>: Notebook interesante sobre Structured Streaming hecho por Databricks \n",
    "    <a target = \"_blank\" href=\"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html\">aquí</a>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset está compuesto por las siguientes variables:\n",
    "\n",
    "1. **Year** 2008\n",
    "2. **Month** 1\n",
    "3. **DayofMonth** 1-31\n",
    "4. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n",
    "5. **DepTime** hora real de salida (local, hhmm)\n",
    "6. **CRSDepTime** hora prevista de salida (local, hhmm)\n",
    "7. **ArrTime** hora real de llegada (local, hhmm)\n",
    "8. **CRSArrTime** hora prevista de llegada (local, hhmm)\n",
    "9. **UniqueCarrier** código del aparato\n",
    "10. **FlightNum** número de vuelo\n",
    "11. **TailNum** identificador de cola: aircraft registration, unique aircraft identifier\n",
    "12. **ActualElapsedTime** tiempo real invertido en el vuelo\n",
    "13. **CRSElapsedTime** en minutos\n",
    "14. **AirTime** en minutos\n",
    "15. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterrizó menos de 15 minutos más tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n",
    "16. **DepDelay** retraso a la salida, en minutos\n",
    "17. **Origin** código IATA del aeropuerto de origen\n",
    "18. **Dest** código IATA del aeropuerto de destino\n",
    "19. **Distance** en millas\n",
    "20. **TaxiIn** taxi in time, in minutes\n",
    "21. **TaxiOut** taxi out time in minutes\n",
    "22. **Cancelled** *si el vuelo fue cancelado (1 = sí, 0 = no)\n",
    "23. **CancellationCode** razón de cancelación (A = aparato, B = tiempo atmosférico, C = NAS, D = seguridad)\n",
    "24. **Diverted** *si el vuelo ha sido desviado (1 = sí, 0 = no)\n",
    "25. **CarrierDelay** en minutos: El retraso del transportista está bajo el control del transportista aéreo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, daño de la aeronave, espera de la llegada de los pasajeros o la tripulación de conexión, equipaje, impacto de un pájaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulación (descanso del piloto o acompañante) , daños por mercancías peligrosas, inspección de ingeniería, abastecimiento de combustible, pasajeros discapacitados, tripulación retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegación de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no válido, retrasos de peso y equilibrio.\n",
    "26. **WeatherDelay** en minutos: causado por condiciones atmosféricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n",
    "27. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorológicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tráfico aéreo, problemas con los controladores aéreos, etc.\n",
    "28. **SecurityDelay** en minutos: causado por la evacuación de una terminal, re-embarque de un avión debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n",
    "29. **LateAircraftDelay** en minutos: debido al propio retraso del avión al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora más tardía de la que estaba prevista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descargamos una versión del dataset reducido dividida en 10 ficheros pequeños y los subimos a la carpeta /tmp/flightsFolder de HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vamos a simular** procesamiento en Streaming leyendo cada vez un fichero de esa carpeta, de los 10 existentes, como si hubiese un proceso externo que los va creando en esa carpeta (aunque ya existan todos desde el principio; por eso los vamos procesando uno a uno). Podéis probar a no subir los 10 sino subir solo 3 o 4, y después de un rato, en cualquier momento subir más ficheros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-27 19:24:56--  https://github.com/olbapjose/xapi-clojure/raw/master/flightsFolder.zip\n",
      "Resolviendo github.com (github.com)... 140.82.121.4\n",
      "Conectando con github.com (github.com)[140.82.121.4]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 302 Found\n",
      "Ubicación: https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flightsFolder.zip [siguiente]\n",
      "--2021-09-27 19:24:57--  https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flightsFolder.zip\n",
      "Resolviendo raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Conectando con raw.githubusercontent.com (raw.githubusercontent.com)[185.199.111.133]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 2728655 (2,6M) [application/zip]\n",
      "Guardando como: “flightsFolder.zip”\n",
      "\n",
      "flightsFolder.zip   100%[===================>]   2,60M  7,10MB/s    en 0,4s    \n",
      "\n",
      "2021-09-27 19:24:59 (7,10 MB/s) - “flightsFolder.zip” guardado [2728655/2728655]\n",
      "\n",
      "Archive:  flightsFolder.zip\n",
      "   creating: flightsFolder/\n",
      "  inflating: flightsFolder/flights0.csv  \n",
      "  inflating: flightsFolder/flights1.csv  \n",
      "  inflating: flightsFolder/flights2.csv  \n",
      "  inflating: flightsFolder/flights3.csv  \n",
      "  inflating: flightsFolder/flights4.csv  \n",
      "  inflating: flightsFolder/flights5.csv  \n",
      "  inflating: flightsFolder/flights6.csv  \n",
      "  inflating: flightsFolder/flights7.csv  \n",
      "  inflating: flightsFolder/flights8.csv  \n",
      "  inflating: flightsFolder/flights9.csv  \n",
      "/bin/bash: hdfs: orden no encontrada\n"
     ]
    }
   ],
   "source": [
    "#!wget https://github.com/olbapjose/xapi-clojure/raw/master/flightsFolder.zip\n",
    "#!unzip flightsFolder.zip\n",
    "#!hdfs dfs -copyFromLocal flightsFolder /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!hdfs dfs -ls /tmp/flightsFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descargamos también una versión completa de ese dataset, que todavía no incluye la columna ArrDelayCat que le vamos a añadir a continuación. Los 10 ficheros anteriores sí tienen ya esa columna y siguen el mismo esquema que este fichero. El objetivo de descargar este fichero es para aprovechar su esquema y no tener que construir nosotros a mano un esquema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-27 19:28:51--  https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flights_jan08.csv\n",
      "Resolviendo raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Conectando con raw.githubusercontent.com (raw.githubusercontent.com)[185.199.110.133]:443... conectado.\n",
      "Petición HTTP enviada, esperando respuesta... 200 OK\n",
      "Longitud: 9719583 (9,3M) [text/plain]\n",
      "Guardando como: “flights_jan08.csv”\n",
      "\n",
      "flights_jan08.csv   100%[===================>]   9,27M  9,23MB/s    en 1,0s    \n",
      "\n",
      "2021-09-27 19:28:53 (9,23 MB/s) - “flights_jan08.csv” guardado [9719583/9719583]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!wget https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flights_jan08.csv\n",
    "#!hdfs dfs -copyFromLocal flights_jan08.csv /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Tema2\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Leemos los datos, quitamos filas con NA y convertimos a numérico\n",
    "flightsDF = spark.read\\\n",
    "                 .option(\"header\", \"true\")\\\n",
    "                 .option(\"inferSchema\", \"true\")\\\n",
    "                 .csv(\"flights_jan08.csv\")\n",
    "\n",
    "cleanFlightsDF = flightsDF.where(\"ArrDelay != 'NA' and DepDelay != 'NA'\")\\\n",
    "                          .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(IntegerType()))\\\n",
    "                          .withColumn(\"DepDelay\", F.col(\"DepDelay\").cast(IntegerType()))\\\n",
    "                          .withColumn(\"ArrDelayCat\", F.when(F.col(\"ArrDelay\") < 15, \"None\")\\\n",
    "                                                      .when((F.col(\"ArrDelay\") >= 15) & (F.col(\"ArrDelay\") < 60), \"Slight\")\\\n",
    "                                                      .otherwise(\"Huge\"))\\\n",
    "                          .cache() # we will be working with it from now on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agregaciones en streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p><b>PREGUNTA</b>: ¿Cuál es el retraso medio por cada destino para vuelos que salen de SFO?\n",
    "    Convierte esta consulta en streaming</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'avg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b263147b289c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Operación de agregación, igual que con un DataFrame convencional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# COMPLETA LA CONSULTA: de los vuelos que salen de SFO, calcular el retraso medio para cada destino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlargestAverageSFOstreamingDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstreamingFlights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Distance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Ahora escribimos continuamente el resultado. Solo para test, escribimos en memoria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'avg'"
     ]
    }
   ],
   "source": [
    "flightsSchema = cleanFlightsDF.schema # in Structured Streaming the schema is mandatory\n",
    "\n",
    "streamingFlights = spark.readStream.schema(flightsSchema)\\\n",
    "                        .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                        .csv(\"flightsFolder\") # leer un solo fichero nuevo en cada operación de lectura de esta carpeta\n",
    "\n",
    "# Operación de agregación, igual que con un DataFrame convencional\n",
    "# COMPLETA LA CONSULTA: de los vuelos que salen de SFO, calcular el retraso medio para cada destino\n",
    "largestAverageSFOstreamingDF = streamingFlights.<COMPLETAR>\n",
    "\n",
    "# Ahora escribimos continuamente el resultado. Solo para test, escribimos en memoria\n",
    "countQuery = largestAverageSFOstreamingDF\\\n",
    "                .writeStream.queryName(\"meanArrDelaySFO\")\\\n",
    "                .format(\"memory\")\\\n",
    "                .outputMode(\"complete\")\\\n",
    "                .start() # esto lanza el stream\n",
    "\n",
    "# Puesto que el driver es este notebook de Jupyter y no lo pensamos cerrar,\n",
    "# hasta que hayamos visualizado correctamente todas las salidas, entonces podemos omitir la línea siguiente\n",
    "\n",
    "#countQuery.awaitTermination() # obligatorio en aplicaciones en producción para evitar que finalice el Driver\n",
    "\n",
    "import time \n",
    "\n",
    "resultDF = spark.sql(\"select * from meanArrDelaySFO\")\n",
    "\n",
    "resultDF.show()\n",
    "\n",
    "time.sleep(15)\n",
    "\n",
    "resultDF.show()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "resultDF.show()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "resultDF.show()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "resultDF.show()\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escribir en memoria implica que se crea una tabla automáticamente en memoria con el nombre de la consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, lo que hemos hecho es que vamos consultando periódicamente el contenido de esa tabla y vemos que cada vez es distinto (va cambiando a lo largo del tiempo a medida que Spark va procesando nuevos ficheros y actualizando el resultado en la tabla)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab89f981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Bloggers\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Bloggers(id:Long, first:String, last:String, url:String, published:String,\n",
    "hits: Long, campaigns:Array[String])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6ad342b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bloggers: String = ../databricks-datasets/learning-spark-v2/blogs.json\n",
       "bloggersDS: org.apache.spark.sql.Dataset[Bloggers] = [Campaigns: array<string>, First: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bloggers = \"../databricks-datasets/learning-spark-v2/blogs.json\"\n",
    "val bloggersDS = spark\n",
    " .read\n",
    " .format(\"json\")\n",
    " .option(\"path\", bloggers)\n",
    " .load()\n",
    " .as[Bloggers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd292fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[Bloggers] = Array(Bloggers(1,Jules,Damji,https://tinyurl.1,1/4/2016,4535,[Ljava.lang.String;@54f6955c), Bloggers(2,Brooke,Wenig,https://tinyurl.2,5/5/2018,8908,[Ljava.lang.String;@6b899e09), Bloggers(3,Denny,Lee,https://tinyurl.3,6/7/2019,7659,[Ljava.lang.String;@5d03a0ce), Bloggers(4,Tathagata,Das,https://tinyurl.4,5/12/2018,10568,[Ljava.lang.String;@224b6709), Bloggers(5,Matei,Zaharia,https://tinyurl.5,5/14/2014,40578,[Ljava.lang.String;@42775a69), Bloggers(6,Reynold,Xin,https://tinyurl.6,3/2/2015,25568,[Ljava.lang.String;@65a9edf8))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloggersDS.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05a70e",
   "metadata": {},
   "source": [
    "#### Working with Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a1e3221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random._\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random._\n",
    "import spark.implicits._  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29a35f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Usage\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Our case class for the Dataset\n",
    "case class Usage(uid:Int, uname:String, usage: Int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a65f7015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "r: scala.util.Random = scala.util.Random@49fb0c76\n",
       "data: scala.collection.immutable.IndexedSeq[Usage] = Vector(Usage(0,user-Gpi2C,525), Usage(1,user-DgXDi,502), Usage(2,user-M66yO,170), Usage(3,user-xTOn6,913), Usage(4,user-3xGSz,246), Usage(5,user-2aWRN,727), Usage(6,user-EzZY1,65), Usage(7,user-ZlZMZ,935), Usage(8,user-VjxeG,756), Usage(9,user-iqf1P,3), Usage(10,user-91S1q,794), Usage(11,user-qHNj0,501), Usage(12,user-7hb94,460), Usage(13,user-bz0WF,142), Usage(14,user-71nwy,479), Usage(15,user-7GZz1,823), Usage(16,user-1CSk6,140), Usage(17,user-WPzlL,246), Usage(18,user-VaEit,451), Usage(19,user-PSaRq,679), Usage(20,user-0Kkzu,332), Usage(21,user-UN3MG,172), Usage(22,user-KwwER,442), Usage(23,user-ZnltJ,923), Usage(24,user-IRA17,741), Usage(25,user-yNHRT,299), Usage(26,user-CJY3C,996)...\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val r = new scala.util.Random(42)\n",
    "// Create 1000 instances of scala Usage class\n",
    "// This generates data on the fly\n",
    "val data = for (i <- 0 to 1000)\n",
    " yield (Usage(i, \"user-\" + r.alphanumeric.take(5).mkString(\"\"), r.nextInt(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "063f43f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|     uname|usage|\n",
      "+---+----------+-----+\n",
      "|  0|user-Gpi2C|  525|\n",
      "|  1|user-DgXDi|  502|\n",
      "|  2|user-M66yO|  170|\n",
      "|  3|user-xTOn6|  913|\n",
      "|  4|user-3xGSz|  246|\n",
      "|  5|user-2aWRN|  727|\n",
      "|  6|user-EzZY1|   65|\n",
      "|  7|user-ZlZMZ|  935|\n",
      "|  8|user-VjxeG|  756|\n",
      "|  9|user-iqf1P|    3|\n",
      "+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dsUsage: org.apache.spark.sql.Dataset[Usage] = [uid: int, uname: string ... 1 more field]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a Dataset of Usage typed data\n",
    "val dsUsage = spark.createDataset(data)\n",
    "dsUsage.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a859c",
   "metadata": {},
   "source": [
    "#### Higher-order functions and functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4cb9764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|uname     |usage|\n",
      "+---+----------+-----+\n",
      "|605|user-NL6c4|999  |\n",
      "|113|user-nnAXr|999  |\n",
      "|634|user-L0wci|999  |\n",
      "|561|user-5n2xY|999  |\n",
      "|26 |user-CJY3C|996  |\n",
      "+---+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "dsUsage\n",
    " //.filter(d => d.usage > 900)\n",
    " .filter($\"usage\" > 900)\n",
    " .orderBy(desc(\"usage\"))\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "18c1ae61",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "2: error: not a legal formal parameter.",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: not a legal formal parameter.",
      "Note: Tuples cannot be directly destructured in method or function parameters.",
      "      Either create a single parameter accepting the Tuple1,",
      "      or consider a pattern matching anonymous function: `{ case (param1, param1) => ... }",
      "       dsUsage.map({if ($\"usage\" == 750) $\"usage\" => $\"usage\" * .15 else $\"usage\" => $\"usage\" * .50 })",
      "                                         ^",
      "<console>:2: error: not a legal formal parameter.",
      "Note: Tuples cannot be directly destructured in method or function parameters.",
      "      Either create a single parameter accepting the Tuple1,",
      "      or consider a pattern matching anonymous function: `{ case (param1, param1) => ... }",
      "       dsUsage.map({if ($\"usage\" == 750) $\"usage\" => $\"usage\" * .15 else $\"usage\" => $\"usage\" * .50 })",
      "                                                                         ^",
      ""
     ]
    }
   ],
   "source": [
    "def filterWithUsage(u: Usage) = u.usage > 900\n",
    "dsUsage.filter(filterWithUsage(_)).orderBy(desc(\"usage\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f5a42d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 115) (192.168.0.11 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 115) (192.168.0.11 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:827)",
      "  ... 54 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "// Use an if-then-else lambda expression and compute a value\n",
    "dsUsage.map(u => {if (u.usage > 750) u.usage * .15 else u.usage * .50 })\n",
    " .show(5, false)\n",
    "// Define a function to compute the usage\n",
    "def computeCostUsage(usage: Int): Double = {\n",
    " if (usage > 750) usage * 0.15 else usage * 0.50\n",
    "}\n",
    "// Use the function as an argument to map()\n",
    "dsUsage.map(u => {computeCostUsage(u.usage)}).show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d22c0514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class UsageCost\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a new case class with an additional field, cost\n",
    "case class UsageCost(uid: Int, uname:String, usage: Int, cost: Double)\n",
    "// Compute the usage cost with Usage as a parameter\n",
    "// Return a new object, UsageCost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ee681fba",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 117) (192.168.0.11 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 117) (192.168.0.11 executor driver): java.lang.ClassCastException: $iw cannot be cast to $iw",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:748)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2722)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2929)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:338)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:784)",
      "  ... 55 elided",
      "Caused by: java.lang.ClassCastException: $iw cannot be cast to $iw",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.deserializetoobject_doConsume_0$(Unknown Source)",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "def computeUserCostUsage(u: Usage): UsageCost = {\n",
    " val v = if (u.usage > 750) u.usage * 0.15 else u.usage * 0.50\n",
    " UsageCost(u.uid, u.uname, u.usage, v)\n",
    "}\n",
    "// Use map() on our original Dataset\n",
    "dsUsage.map(u => {computeUserCostUsage(u)}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c6256",
   "metadata": {},
   "source": [
    "#### Conversión de DataFrames a Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "457f1a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bloggers: String = ../databricks-datasets/learning-spark-v2/blogs.json\n",
       "bloggersDS: org.apache.spark.sql.Dataset[Bloggers] = [Campaigns: array<string>, First: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bloggers = \"../databricks-datasets/learning-spark-v2/blogs.json\"\n",
    "val bloggersDS = spark\n",
    " .read\n",
    " .format(\"json\")\n",
    " .option(\"path\", bloggers)\n",
    " .load()\n",
    " .as[Bloggers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "740f35c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Person\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(id: Integer, firstName: String, middleName: String, lastName: String,\n",
    "gender: String, birthDate: String, ssn: String, salary: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5e956f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "7: error: ')' expected but '=>' found.",
     "output_type": "error",
     "traceback": [
      "<console>:7: error: ')' expected but '=>' found.",
      "        .filter(x => x.birthDate.split(\"-\")(0).toInt > earliestYear)",
      "                  ^",
      "<console>:7: error: '=' expected but '(' found.",
      "        .filter(x => x.birthDate.split(\"-\")(0).toInt > earliestYear)",
      "                                           ^",
      ""
     ]
    }
   ],
   "source": [
    "import java.util.Calendar\n",
    "val earliestYear = Calendar.getInstance.get(Calendar.YEAR) - 40\n",
    "\n",
    "val personDS\n",
    " // Everyone above 40: lambda-1\n",
    " .filter(x => x.birthDate.split(\"-\")(0).toInt > earliestYear)\n",
    " // Everyone earning more than 80K\n",
    " .filter($\"salary\" > 80000)\n",
    "// Last name starts with J: lambda-2\n",
    " .filter(x => x.lastName.startsWith(\"J\"))\n",
    " // First name starts with D\n",
    " .filter($\"firstName\".startsWith(\"D\"))\n",
    " .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9091372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

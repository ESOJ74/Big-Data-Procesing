{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21ef771b",
   "metadata": {},
   "source": [
    "\n",
    "# MNIST Image Classification with TensorFlow\n",
    "This notebook demonstrates how to implement a simple linear image model on MNIST using the tf.keras API. It builds the foundation for this companion notebook, which explores tackling the same problem with other types of models such as DNN and CNN.\n",
    "\n",
    "This notebook uses TF2.0 Please check your tensorflow version using the cell below. If it is not 2.0, please run the pip line below and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fb2b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "291d6c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import unittest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Softmax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae66e8f",
   "metadata": {},
   "source": [
    "# Exploring the data\n",
    "The MNIST dataset is already included in tensorflow through the keras datasets module. Let's load it and get a sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "320c58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa4d419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image height x width is 28 x 28\n",
      "There are 10 classes\n"
     ]
    }
   ],
   "source": [
    "HEIGHT, WIDTH = x_train[0].shape\n",
    "NCLASSES = tf.size(tf.unique(y_train).y)\n",
    "print(\"Image height x width is\", HEIGHT, \"x\", WIDTH)\n",
    "tf.print(\"There are\", NCLASSES, \"classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e960c",
   "metadata": {},
   "source": [
    "Each image is 28 x 28 pixels and represents a digit from 0 to 9. These images are black and white, so each pixel is a value from 0 (white) to 255 (black). Raw numbers can be hard to interpret sometimes, so we can plot the values to see the handwritten digit as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fe6c747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label for image number 12 is 9\n"
     ]
    }
   ],
   "source": [
    "IMGNO = 12\n",
    "# Uncomment to see raw numerical values.\n",
    "# print(x_test[IMGNO])\n",
    "plt.imshow(x_test[IMGNO].reshape(HEIGHT, WIDTH));\n",
    "print(\"The label for image number\", IMGNO, \"is\", y_test[IMGNO])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca9993",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "Let's start with a very simple linear classifier. This was the first method to be tried on MNIST in 1998, and scored an 88% accuracy. Quite ground breaking at the time!\n",
    "\n",
    "We can build our linear classifer using the tf.keras API, so we don't have to define or initialize our weights and biases. This happens automatically for us in the background. We can also add a softmax layer to transform the logits into probabilities. Finally, we can compile the model using categorical crossentropy in order to strongly penalize high probability predictions that were incorrect.\n",
    "\n",
    "When building more complex models such as DNNs and CNNs our code will be more readable by using the tf.keras API. Let's get one working so we can test it and use it as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fa6dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model():\n",
    "    model = Sequential([\n",
    "        Flatten(),\n",
    "        Dense(NCLASSES),\n",
    "        Softmax()\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586bed4",
   "metadata": {},
   "source": [
    "### Write Input Functions\n",
    "As usual, we need to specify input functions for training and evaluating. We'll scale each pixel value so it's a decimal value between 0 and 1 as a way of normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "707fe25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 5000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TRAIN_KEY = tf.estimator.ModeKeys.TRAIN\n",
    "EVAL_KEY = tf.estimator.ModeKeys.EVAL\n",
    "\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def load_dataset(mode):\n",
    "    dataset = (x_train, y_train) if mode == TRAIN_KEY else (x_test, y_test)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.map(scale).batch(BATCH_SIZE)\n",
    "    if mode == TRAIN_KEY:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60e9b7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for train passed!\n",
      "Test for eval passed!\n"
     ]
    }
   ],
   "source": [
    "def create_shape_test(key):\n",
    "    dataset = load_dataset(key)\n",
    "    data_iter = dataset.__iter__()\n",
    "    (images, labels) = data_iter.get_next()\n",
    "    expected_image_shape = (BATCH_SIZE, HEIGHT, WIDTH)\n",
    "    expected_label_shape = (BATCH_SIZE)\n",
    "    assert(images.shape == expected_image_shape)\n",
    "    assert(labels.shape == expected_label_shape)\n",
    "    print(\"Test for\", key, \"passed!\")\n",
    "\n",
    "\n",
    "create_shape_test(TRAIN_KEY)\n",
    "create_shape_test(EVAL_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bd645",
   "metadata": {},
   "source": [
    "\n",
    "Time to train the model! The original MNIST linear classifier had an error rate of 12%. Let's use that to sanity check that our model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0333c407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 - 2s - loss: 1.3504 - accuracy: 0.6583 - val_loss: 0.7972 - val_accuracy: 0.8389\n",
      "\n",
      "Epoch 00001: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 2/10\n",
      "100/100 - 0s - loss: 0.6743 - accuracy: 0.8493 - val_loss: 0.5598 - val_accuracy: 0.8730\n",
      "\n",
      "Epoch 00002: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 3/10\n",
      "100/100 - 0s - loss: 0.5180 - accuracy: 0.8727 - val_loss: 0.4676 - val_accuracy: 0.8862\n",
      "\n",
      "Epoch 00003: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 4/10\n",
      "100/100 - 0s - loss: 0.4764 - accuracy: 0.8731 - val_loss: 0.4245 - val_accuracy: 0.8905\n",
      "\n",
      "Epoch 00004: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 5/10\n",
      "100/100 - 0s - loss: 0.4211 - accuracy: 0.8882 - val_loss: 0.3902 - val_accuracy: 0.9015\n",
      "\n",
      "Epoch 00005: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 6/10\n",
      "100/100 - 0s - loss: 0.4056 - accuracy: 0.8935 - val_loss: 0.3701 - val_accuracy: 0.9045\n",
      "\n",
      "Epoch 00006: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 7/10\n",
      "100/100 - 0s - loss: 0.3811 - accuracy: 0.9000 - val_loss: 0.3550 - val_accuracy: 0.9062\n",
      "\n",
      "Epoch 00007: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 8/10\n",
      "100/100 - 0s - loss: 0.3594 - accuracy: 0.9016 - val_loss: 0.3470 - val_accuracy: 0.9058\n",
      "\n",
      "Epoch 00008: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 9/10\n",
      "100/100 - 0s - loss: 0.3413 - accuracy: 0.9072 - val_loss: 0.3321 - val_accuracy: 0.9102\n",
      "\n",
      "Epoch 00009: saving model to mnist/linear_keras/checkpoints/mnist\n",
      "Epoch 10/10\n",
      "100/100 - 0s - loss: 0.3500 - accuracy: 0.9035 - val_loss: 0.3243 - val_accuracy: 0.9123\n",
      "\n",
      "Epoch 00010: saving model to mnist/linear_keras/checkpoints/mnist\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 100\n",
    "\n",
    "model = linear_model()\n",
    "train_data = load_dataset(TRAIN_KEY)\n",
    "validation_data = load_dataset(EVAL_KEY)\n",
    "\n",
    "OUTDIR = \"mnist/linear_keras\"\n",
    "checkpoint_path = '{}/checkpoints/mnist'.format(OUTDIR)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path, save_weights_only=True, verbose=1)\n",
    "tensorboard_callback = TensorBoard(log_dir=OUTDIR)\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    validation_data=validation_data,\n",
    "    epochs=NUM_EPOCHS, \n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint_callback, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae73c0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test to beat benchmark accuracy passed!\n",
      "Test model accuracy is improving passed!\n",
      "Test loss is decreasing passed!\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_ERROR = .12\n",
    "BENCHMARK_ACCURACY = 1 - BENCHMARK_ERROR\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "    \n",
    "assert(accuracy[-1] > BENCHMARK_ACCURACY)\n",
    "print(\"Test to beat benchmark accuracy passed!\")\n",
    "        \n",
    "assert(accuracy[0] < accuracy[1])\n",
    "assert(accuracy[1] < accuracy[-1])\n",
    "print(\"Test model accuracy is improving passed!\")\n",
    "    \n",
    "assert(loss[0] > loss[1])\n",
    "assert(loss[1] > loss[-1])\n",
    "print(\"Test loss is decreasing passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea1936c",
   "metadata": {},
   "source": [
    "## Evaluating Predictions\n",
    "Let's check Tensorboard to visually see how the model's accuracy and loss change over time. If running on a Deep Learning VM, wait for\n",
    "\n",
    "TensorBoard <version> at http://<notebook name>:6006/\n",
    "\n",
    "to appear, then go to File > New Launcher. Click on Tensorboard under \"Other\". Interrupt the kernel when you are ready to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee0cd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OUTDIR\"] = OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f142eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-05 11:26:08.708915: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-05 11:26:08.708934: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-10-05 11:26:10.663459: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-10-05 11:26:10.663485: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jose-HP-All-in-One-24-df0xxx): /proc/driver/nvidia/version does not exist\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=$OUTDIR --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b4e712",
   "metadata": {},
   "source": [
    "Were you able to get an accuracy of over 90%? Not bad for a linear estimator! Let's make some predictions and see if we can find where the model has trouble. Change the range of values below to find incorrect predictions, and plot the corresponding images. What would you have guessed for these images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76bace5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasettraining' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dfd5fc169954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatasettraining\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0manalyst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpredicted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_prediction_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpredicted_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-dfd5fc169954>\u001b[0m in \u001b[0;36mload_prediction_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_numbers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatasettraining\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0manalyst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpredicted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_prediction_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasettraining' is not defined"
     ]
    }
   ],
   "source": [
    "image_numbers = range(0, 10, 1)  # Change me, please.\n",
    "\n",
    "def load_prediction_dataset():\n",
    "    dataset = (x_test[image_numbers], y_test[image_numbers])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.map(scale).batch(len(image_numbers))\n",
    "    return datasettraining-data-analyst\n",
    "\n",
    "predicted_results = model.predict(load_prediction_dataset())\n",
    "for index, prediction in enumerate(predicted_results):\n",
    "    predicted_value = np.argmax(prediction)\n",
    "    actual_value = y_test[image_numbers[index]]\n",
    "    if actual_value != predicted_value:\n",
    "        print(\"image number: \" + str(image_numbers[index]))\n",
    "        print(\"the prediction was \" + str(predicted_value))\n",
    "        print(\"the actual label is \" + str(actual_value))\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_image_number = 8\n",
    "plt.imshow(x_test[bad_image_number].reshape(HEIGHT, WIDTH));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e67a20",
   "metadata": {},
   "source": [
    "It's understandable why the poor computer would have some trouble. Some of these images are difficult for even humans to read. Ready for the next challenge? Click here to super charge our models with human-like vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca05e3",
   "metadata": {},
   "source": [
    "#### Bonus Exercise\n",
    "Want to push your understanding further? Instead of using Keras'es built in layers, try repeating the above exercise with your own custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ac5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

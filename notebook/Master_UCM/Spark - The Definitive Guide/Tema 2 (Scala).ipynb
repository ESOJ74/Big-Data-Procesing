{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introducción:\n",
    "\n",
    "## 2.1. Spark Session\n",
    "\n",
    "La aplicación de Spark se controla a través de un controlador llamado SparkSession. La instancia SparkSession es la forma en que se ejecuta Spark y el código definido por el usuario. Siempre debemos iniciar una instancia SparkSession al principio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos saber la versión que estamos utilizando de Spark podemos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos una simple tarea que consistirá en la creación de un rango de números que será como una columna de números en una hoja de cálculo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mi_Rango: org.apache.spark.sql.DataFrame = [numero: bigint]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mi_Rango = spark.range(1000).toDF(\"numero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. DataFrames:\n",
    "\n",
    "Un DataFrame es la API estructurada más común y simplemente representa una tabla de datos con filas y columnas. La lista que define las columnas y los tipos dentro de esas columnas se llama el esquema. Se puede pensar en un DataFrame como una hoja de cálculo de columnas con nombre. La diferencia fundamental es que una hoja de cálculo se encuentra en una computadora, mientras que un Spark DataFrame puede estar dividida en miles de computadoras. La razón para poner los datos en más de un ordenador es sencillo, los datos son demasiado grandes para caber en una sola máquina o simplemente llevaría demasiado tiempo realizar ese cálculo en una sola máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Particiones:\n",
    "\n",
    "Para que cada executor pueda realizar su trabajo en paralelo con el resto, Spark divide los datos en trozos más pequeños llamados particiones. Una partición es una colección de filas que están en una máquina física en el clúster. Las particiones del DataFrame representan cómo se distribuyen físicamente los datos en las máquinas del cluster durante la ejecución. Si solo se tiene una partición, Spark tendrá un paralelismo de solo uno, aunque tengamos miles de executors. De la misma forma si tenemos muchas particiones pero, solo un executor, Spark seguirá teniendo un paralelismo de solo uno. \n",
    "\n",
    "Una cosa importante a tener en cuenta es que cuando trabajamos con DataFrames no manipulamos las particiones de forma manual (la mayoría de las veces). Simplemente definimos las transformaciones que queremos y Spark determina como se ejecutará a nivel interno en el clúster. Existe API de nivel inferior a través de RDDs que veremos más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Transformaciones:\n",
    "\n",
    "Spark utiliza lo que se conoce como evaluación perezosa. Esto significa que no hará ningún trabajo, a menos que realmente tenga que hacerlo. Este enfoque permite evitar el uso innecesario de recursos, además de permitir una mejor optimización del proceso. Las llamadas transformaciones, se evalúan de forma perezosa y solo se ejecutará realmente cuando se produzca una acción que definiremos en breve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "divisoresDe2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [numero: bigint]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val divisoresDe2 = mi_Rango.where(\"numero % 2 = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en el código anterior, no hemos obtenido ningún resultado. Esto es porque solo hemos especificado una transformación y Spark no la ejecutará hasta que ejecutemos una acción. Digamos que Spark va \"apuntando\" las diferentes transformaciones que deseamos hacer sobre el DataFrame, una vez ejecutemos una acción optimizará todas las diferentes transformaciones y las llevara a cabo.\n",
    "\n",
    "Hay que diferenciar dos tipos de transformaciones, las que se dice que tienen dependencias estrechas, **Narrow transformations**, y las que tienen amplias dependencias, **Wide transformations (Shuffles)**. \n",
    "\n",
    "<center><img src=\"./images/201.jpg\"></center>\n",
    "\n",
    "Las transformaciones Narrow como se puede ver en la imagen, son aquellas en las que cada partición de entrada contribuirá a una sola partición de salida. En el anterior fragmento de código, se trata de un tipo Narrow, donde la \"filtración\" de números divisores de 2 pueden hacerse en cada partición (o nodo) de forma independiente sin intercambio de datos.\n",
    "\n",
    "<center><img src=\"./images/202.jpg\"></center>\n",
    "    \n",
    "Por otra parte, una transformación Shuffle, tendrá particiones de entrada que contribuyen a varias particiones de salida. Podemos pensar por ejemplo en transformaciones que necesiten hacer operaciones entre columnas y por lo tanto tenga que haber intercambio de datos entre las diferentes particiones (o nodos).\n",
    "\n",
    "En las operaciones de tipo Narrow, Spark las ejecuta por medio de pipelines de manera que si especificamos varios filtros, los realizará todos en memoria. En el caso de las transformaciones de tipo Shuffle esto no es posible, ya que habrá intercambio de datos entre particiones y tendrá que hacer escritura en disco."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. Evaluación Perezosa (Lazy)\n",
    "\n",
    "La evaluación perezosa significa que Spark esperará hasta el último momento para ejecutar el grafo de instrucciones de cálculo. En lugar de ir modificando los datos inmediatamente después de cada operación, elabora un esquema de transformaciones que deberá aplicar. En el momento que vaya a ejecutarlo tendrá todo el proceso optimizado de la manera más eficiente posible.\n",
    "\n",
    "Podemos pensar en una serie de transformaciones en las que en el último paso filtramos una fila del DataFrame. Spark al analizar y optimizar las diferentes transformaciones, filtrará esa fila al principio, haciendo que trabajemos con menos datos y de una forma más rápida y eficiente. Spark hará esto de forma autónoma y transparente a nosotros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Acciones:\n",
    "\n",
    "Mientras que las transformaciones son las que generan el esquema lógico, las acciones son las que desencadenan que Spark calcule un cierto resultado ejecutando todas las transformaciones definidas anteriormente. \n",
    "\n",
    "Por ejemplo podemos contar el número de divisores de 2 en nuestro DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Long = 500\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisoresDe2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos efectuado una acción, como en este caso count(), es cuando realmente se van a ejecutar todas las transformaciones anteriores. Por supuesto, count() no es la única acción. Existen tres tipos de acciones:\n",
    "\n",
    "$\\bullet$ Acciones para ver datos en la consola.  \n",
    "$\\bullet$ Acciones para recopilar datos en objetos nativos en el respectivo lenguaje.  \n",
    "$\\bullet$ Acciones para escribir en fuentes de datos externas.\n",
    "\n",
    "Al ejecutar la acción, iniciamos el job de Spark que ejecuta nuestra transformación de filtro where (una transformación de tipo Narrow), luego una agregación (una transformación de tipo Shuffle) que realiza el recuento en cada de partición, y luego una recopilación, que lleva nuestro resultado a un objeto nativo en el respectivo idioma. Todo esto se puede ver en la Spark UI, que es una herramienta de supervisión de los trabajos que se ejecutan en el cluster. A esta herramienta se accede a través de http://localhost:4040 por defecto cuando trabajamos de forma local. \n",
    "\n",
    "**NOTA:** Si creamos varias SparkSession se irán creando en puertos sucesivos a partir del 4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Ejemplo General:\n",
    "\n",
    "En los ejemplos anteriores creamos un dataframe de un rango de números, esto realmente no es un trabajo de Big Data. En esta sección reforzaremos los conceptos aprendidos en el tema con un ejemplo más realista y explicaremos paso por paso que es lo que ocurre realmente. Usaremos un archivo sobre datos de la oficina de estadística de transportes de los Estados Unidos. Utilizaremos el archivo 2015-summary.csv que se encuentra en la carpeta Datasets.\n",
    "\n",
    "Los archivos CSV son un tipo de archivos de datos semiestructurados donde cada fila en el CSV representara una fila de nuestro DataFrame. Spark tiene la habilidad de leer y escribir en un diverso tipo de fuentes de datos. En este caso para leer el archivo haremos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightData2015: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightData2015 = spark.read\n",
    "                      .option(\"inferSchema\", \"true\")\n",
    "                      .option(\"header\", \"true\")\n",
    "                      .csv(\"2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código podemos ver como en la primera opción $\\textit{inferSchema}$ lo que hacemos es lo que se llama inferir esquema, que no es otra cosa que decirle a Spark que intente saber que tipo de dato tiene cada columna. En la segunda opción $\\textit{header}$ le decimos a Spark que la primera fila se trata del nombre de las columnas y no de datos. \n",
    "\n",
    "Este DataFrame inicial, realmente tiene un conjunto de columnas con un número indeterminado de filas. La razón por la que no se especifica el número de filas es porque la lectura de datos es una transformación, y como ya hemos comentado se trata de una operación perezosa. Spark lee solo un par de filas de datos para intentar adivinar de que tipo debería ser cada columna. De todas formas, lo más recomendable sería indicarle de forma explícita el esquema de datos, lo que veremos más adelante.\n",
    "\n",
    "Para tener una imagen mental del proceso que acabamos de llevar a cabo, observemos la siguiente figura:\n",
    "\n",
    "<center><img src=\"./images/203.jpg\"></center>\n",
    "\n",
    "Como se puede observar, leemos el archivo CSV como un DataFrame y luego se convierte en una matriz de filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344])\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, ordenaremos por medio de $\\textit{sort}$ nuestro DataFrame respecto a la columna \"count\" que es de tipo entero.\n",
    "\n",
    "**NOTA:** Ten en cuenta que el ordenamiento de un DataFrame no modifica el DataFrame de partida. Ordenar se trata de una transformación que devuelve un nuevo DataFrame a partir de transformar el anterior. Visualmente:\n",
    "\n",
    "<center><img src=\"./images/204.jpg\"></center>\n",
    "\n",
    "No sucede nada con los datos cuando ordenamos porque es solo una transformación. Sin embargo, podemos ver que Spark está elaborando un plan sobre cómo ejecutará esto en el cluster. Podemos utilizar la función $\\textit{explain}$ en cualquier objeto Dataframe para ver como Spark ejecutará el código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#42 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#42 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#69]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#40,ORIGIN_COUNTRY_NAME#41,count#42] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/media/jose/Repositorio/Git_Hub/Curso-Apache-Spark-Castellano/Notebooks/Sp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La parte superior es el resultado final y la parte inferior la fuente de datos. En este caso concreto, podemos ver las palabras clave de cada proceso, \"Sort\", \"Exchange\" y \"Filescan\". Esto se debe a que el ordenamiento es una transformación de tipo **Shuffle**, ya que cada dato se debe comparar con el resto para la ordenación. Ahora mismo no es importante entender todo esto, ya que se trata de una herramienta útil para la depuración y mejorar en el conocimiento a medida que se avanza en la formacion en Spark.\n",
    "\n",
    "Ahora, podemos ejecutar una acción para poner en marcha este plan. Sin embargo, antes vamos a hacer una configuración. De forma predeterminada, cuando realizamos un **shuffle**, Spark hace 200 particiones en orden aleatorio. Vamos a cambiar este valor a 5 para reducir el número de particiones de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [count#42 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#42 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#81]\n",
      "   +- FileScan csv [DEST_COUNTRY_NAME#40,ORIGIN_COUNTRY_NAME#41,count#42] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/media/jose/Repositorio/Git_Hub/Curso-Apache-Spark-Castellano/Notebooks/Sp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repitiendo el código anterior podemos comprobar como en \"Exchange\", donde se indica el número de particiones, el segundo argumento que antes era 200 ahora es 5. Esto solo se indica como nota para que vayamos entendiendo la diferente información que nos muestra \"explain\".\n",
    "\n",
    "Ahora le diremos que nos muestre los 2 primeros valores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Array[org.apache.spark.sql.Row] = Array([Moldova,United States,1], [United States,Croatia,1], [United States,Singapore,1])\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proceso se ilustra en la siguiente figura:\n",
    "\n",
    "<center><img src=\"./images/205.jpg\"></center>\n",
    "\n",
    "Spark genera un plan de transformaciones de Dataframes de manera que en cualquier momento puede volver a un DataFrame anterior simplemente realizando los cálculos necesarios a partir del DataFrame de partida. Esto es parte del corazón de Spark, donde no modificamos los datos de forma física, sino que solo creamos un \"mapa\" de las transformaciones en cada paso. Un ejemplo de esto es la selección del número de particiones, que por defecto eran 200 y hemos configurado en 5. Si jugamos con el número de particiones y analizamos el tiempo de ejecución, veremos que varía drásticamente. Todo esto lo podemos analizar de forma detallada a través del Spark UI (<http://localhost:4040>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.1. DataFrames y SQL:\n",
    "\n",
    "En el ejemplo anterior solo vimos un ejemplo sencillo. Ahora haremos un ejemplo más complejo tratándolo como un DataFrame y como SQL puro. Spark puede trabajar de la misma forma independientemente del idioma. Podemos hacer la programación como DataFrames (ya sea con R, Python, Scala o Java) o como SQL puro. Spark compilará la lógica de la misma forma independientemente del lenguaje antes de ejecutar el código. Con Spark SQL podemos hacer una vista (o tabla temporal) y consultarla usando SQL puro. Además, no habrá diferencia en rendimiento lo hagamos de una u otra forma, ya que ambos se \"compilarán\" de la misma forma en el diseño del \"mapa\" de ejecución.\n",
    "\n",
    "Podemos convertir cualquier DataFrame en una tabla o vista con el siguiente método:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos hacer cualquier consulta en SQL. Para ello usaremos la función spark.sql (recuerda que spark es nuestra variable SparkSession) que devolverá un DataFrame. Aunque puede parecer un método circular, ya que una consulta en SQL devuelve un Dataframe; en realidad, es bastante poderoso. Esto hace que sea posible especificar las transformaciones de la manera más conveniente sin sacrificar la eficiencia. Para entender esto observemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#40], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#40, 5), ENSURE_REQUIREMENTS, [id=#191]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#40], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#40] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/media/jose/Repositorio/Git_Hub/Curso-Apache-Spark-Castellano/Notebooks/Sp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#40], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#40, 5), ENSURE_REQUIREMENTS, [id=#210]\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#40], functions=[partial_count(1)])\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#40] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/media/jose/Repositorio/Git_Hub/Curso-Apache-Spark-Castellano/Notebooks/Sp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count(1): bigint]\n",
       "dataFrameWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlWay = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(1)\n",
    "                      FROM flight_data_2015\n",
    "                      GROUP BY DEST_COUNTRY_NAME\"\"\")\n",
    "\n",
    "val dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\n",
    "                             .count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, ambos códigos producen exactamente el mismo \"mapa\" de ejecución.\n",
    "\n",
    "Ahora usaremos la función max, para calcular el número máximo de vuelos hacia o desde cualquier ubicación. Esto lo que hará será analizar cada valor de la columna relevante en el DataFrame y verificar si ese valor es mayor que los anteriores. Esto es una transformación, ya que solo estamos filtrando una fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[org.apache.spark.sql.Row] = Array([370002])\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.max\n",
       "res28: Array[org.apache.spark.sql.Row] = Array([370002])\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.max\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, tanto de una u otra manera el resultado es el mismo. Ahora buscaremos los cinco principales países de destino. Esta será la primera consulta con varias transformaciones que haremos paso a paso. Empezaremos con SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "maxSql: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, destination_total: bigint]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxSql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "                      FROM flight_data_2015\n",
    "                      GROUP BY DEST_COUNTRY_NAME\n",
    "                      ORDER BY sum(count) DESC\n",
    "                      LIMIT 5\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora lo haremos por medio de la sintaxis de DataFrame que es semánticamente similar pero ligeramente diferente en\n",
    "implementación y ordenación. Pero, como ya hemos comentado, los planes subyacentes para ambos son iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.desc\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.desc\n",
    "\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\n",
    "              .sum(\"count\")\n",
    "              .withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    "              .sort(desc(\"destination_total\"))\n",
    "              .limit(5)\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código tiene siete pasos que nos llevan de regreso a los datos de origen. \n",
    "\n",
    "<center><img src=\"./images/206.jpg\"></center>\n",
    "\n",
    "El verdadero plan de ejecución (el que veríamos si hacemos \"explain\") será diferente del que mostramos en la figura. Esto se debe a las optimizaciones que realiza Spark. Estos planos de ejecución es lo que se llama DAG (Gráfico Acíclico Dirigido) de transformaciones, donde en el momento que ejecutamos una acción se genera el resultado.\n",
    "\n",
    "El primer paso que observamos es la lectura de los datos. Definimos el DataFrame anteriormente, pero como recordatorio, Spark en realidad no lee los datos hasta que se invoca una acción en ese DataFrame o DataFrame derivado del original. El segundo paso es nuestra agrupación; técnicamente cuando llamamos a groupBy, terminamos con un RelationalGroupedDataset, que es una forma elegante de llamar a un DataFrame que tiene una agrupación especifica. Nosotros básicamente especificamos que vamos a agrupar por una clave (o un conjunto de claves) y que vamos a realizar una agregación sobre cada una de esas claves.\n",
    "\n",
    "Por lo tanto, el tercer paso es especificar la agregación. En este caso usamos el método de agregación suma. Esto toma como entrada una columna o, simplemente, un nombre de columna. El resultado de la suma es un nuevo DataFrame. Es importante recordar (otra vez) que no se ha realizado ningún cálculo aun. Esta es simplemente otra transformación que hemos expresado, y Spark simplemente la añade al plan de ejecución.\n",
    "\n",
    "El cuarto paso es un simple cambio de nombre. Usamos el método withColumnRenamed que toma dos argumentos, el nombre de la columna original y el nombre de la nueva columna. Por supuesto, esto es otra transformación sin ejecución de ningún tipo de cálculo.\n",
    "\n",
    "El quinto paso ordena los datos de tal manera que si tuviéramos que tomar los resultados de la parte superior del DataFrame,\n",
    "tendrían los valores más grandes en la columna destination_total.\n",
    "\n",
    "Como has podido apreciar, hemos tenido que importar la función desc para hacer esto. Y como se ha podido observar, desc no devuelve un string sino una columna. En general muchos métodos de DataFrames aceptarán strings (como los nombres de las columnas), tipos de columnas o expresiones. Realmente columnas y expresiones son lo mismo.\n",
    "\n",
    "En el penúltimo paso, especificaremos un límite. Esto determina que solo queremos devolver los cinco primeros valores de nuestro DataFrame final, en lugar de todos los datos.\n",
    "\n",
    "El último paso es nuestra acción. Ahora es cuando empieza el proceso de recolección de los resultados de nuestro DataFrame, y Spark nos dará una lista o array en el lenguaje en el que lo estemos ejecutando. Para repasar todo esto haremos un explain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#240L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#40,destination_total#240L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#40], functions=[sum(cast(count#42 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#40, 5), ENSURE_REQUIREMENTS, [id=#418]\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#40], functions=[partial_sum(cast(count#42 as bigint))])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#40,count#42] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/media/jose/Repositorio/Git_Hub/Curso-Apache-Spark-Castellano/Notebooks/Sp..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\")\n",
    "              .sum(\"count\")\n",
    "              .withColumnRenamed(\"sum(count)\", \"destination_total\")\n",
    "              .sort(desc(\"destination_total\"))\n",
    "              .limit(5)\n",
    "              .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el resultado del explain no coincide con el \"plan conceptual\" de forma exacta, todos los pasos están realmente ahí. En la primera línea podemos observar el límite de 5 y el ordenamiento por medio de orderBy. También se puede observar que la agregación se realiza en dos pasos, en las llamadas sumas parciales. Esto se debe a que sumar una lista de números tiene la propiedad conmutativa y por lo tanto, Spark puede realizar la suma partición a partición. Y también en último lugar se puede ver la lectura del archivo CSV.\n",
    "\n",
    "Además de mostrar los datos, también los podemos guardar en un archivo que Spark soporte. Por ejemplo, podríamos querer guardarlo en una base de datos PostgreSQL o en otro tipo de archivo.\n",
    "\n",
    "## 2.6. Conclusión:\n",
    "\n",
    "En este capítulo introducimos la base de Apache Spark. Hemos hablado sobre las transformaciones, acciones y de como Spark ejecuta de forma perezosa un DAG de transformaciones en un cierto orden para optimizar el plan de ejecución. También hemos explicado como los datos se organizan en particiones y se establece el escenario para transformaciones más complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3003df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.packages = [\"org.apache.spark:spark-avro_2.12:3.1.2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c270244",
   "metadata": {},
   "source": [
    "# Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492cb0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.11:4046\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1633072755864)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mnm_file: String = ../chapter2/mnm_dataset.csv\n",
       "mnm_df: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mnm_file = \"../chapter2/mnm_dataset.csv\"\n",
    "val mnm_df = (spark.read.format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(mnm_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e9d891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfb225e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.types.StructType = StructType(StructField(State,StringType,true), StructField(Color,StringType,true), StructField(Count,IntegerType,true))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnm_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f6d03a",
   "metadata": {},
   "source": [
    "# Cuando se define un schema al definir un campo por ejemplo\n",
    "StructField('Delay', FloatType(), True) ¿qué significa el último\n",
    "parámetro Boolean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4736fdc",
   "metadata": {},
   "source": [
    "Si acepta o no valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe64b35",
   "metadata": {},
   "source": [
    "# Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32f9c0",
   "metadata": {},
   "source": [
    "https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97974534",
   "metadata": {},
   "source": [
    "# Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y guardar los datos en los formatos:\n",
    "\n",
    "* i. JSON\n",
    "* ii. CSV (dándole otro nombre para evitar sobrescribir el fichero origen)\n",
    "* iii. AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e23765bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonPath: String = json_scala\n",
       "csvPath: String = csv_scala\n",
       "avroPath: String = avro_scala\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonPath = \"json_scala\"\n",
    "val csvPath = \"csv_scala\"\n",
    "val avroPath = \"avro_scala\"\n",
    "mnm_df.write.format(\"json\").save(jsonPath)\n",
    "mnm_df.write.format(\"csv\").save(csvPath)\n",
    "mnm_df.write.format(\"avro\").save(avroPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154b2f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mnm_file: String = avro_scala\n",
       "mnm_df2: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mnm_file = \"avro_scala\"\n",
    "val mnm_df2 = (spark.read.format(\"avro\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(mnm_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "539ea3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[org.apache.spark.sql.Row] = Array([TX,Red,20], [NV,Blue,66], [CO,Blue,79], [OR,Blue,71], [WA,Yellow,93])\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnm_df2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c041a39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

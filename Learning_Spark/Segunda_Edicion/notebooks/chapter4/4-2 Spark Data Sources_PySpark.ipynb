{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002acbac",
   "metadata": {},
   "source": [
    "Spark Data Sources\n",
    "This notebook shows how to use Spark Data Sources Interface API to read file formats:\n",
    "\n",
    "* Parquet\n",
    "* JSON\n",
    "* CSV\n",
    "* Avro\n",
    "* ORC\n",
    "* Image\n",
    "* Binary\n",
    "\n",
    "A full list of DataSource methods is available here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa75f0e",
   "metadata": {},
   "source": [
    "Define paths for the various data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8beb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = \"../../databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet\"\n",
    "json_file = \"../../databricks-datasets/learning-spark-v2/flights/summary-data/json/*\"\n",
    "csv_file = \"../../databricks-datasets/learning-spark-v2/flights/summary-data/csv/*\"\n",
    "orc_file = \"../../databricks-datasets/learning-spark-v2/flights/summary-data/orc/*\"\n",
    "avro_file = \"../../databricks-datasets/learning-spark-v2/flights/summary-data/avro/*\"\n",
    "schema = \"DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9314704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#create a SparkSession\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"Example-3_6\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.1.2\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726b05a",
   "metadata": {},
   "source": [
    "## Parquet Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "      .format(\"parquet\")\n",
    "      .option(\"path\", parquet_file)\n",
    "      .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df60ba3",
   "metadata": {},
   "source": [
    "Another way to read this same data using a variation of this API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346a7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b34ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f3a10",
   "metadata": {},
   "source": [
    "## Use SQL\n",
    "This will create an unmanaged temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl\n",
    "    USING parquet\n",
    "    OPTIONS (\n",
    "      path \"/databricks-datasets/definitive-guide/data/flight-data/parquet/2010-summary.parquet\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9fe72",
   "metadata": {},
   "source": [
    "Use SQL to query the table\n",
    "\n",
    "The outcome should be the same as one read into the DataFrame above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5db70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM us_delay_flights_tbl\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa3abf9",
   "metadata": {},
   "source": [
    "## JSON Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").option(\"path\", json_file).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828930b",
   "metadata": {},
   "source": [
    "## CSV Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark\n",
    "      .read\n",
    "\t .format(\"csv\")\n",
    "\t .option(\"header\", \"true\")\n",
    "\t .schema(schema)\n",
    "\t .option(\"mode\", \"FAILFAST\")  # exit if any errors\n",
    "\t .option(\"nullValue\", \"\")\t  # replace any null data field with “”\n",
    "\t .option(\"path\", csv_file)\n",
    "\t .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.write.format(\"parquet\")\n",
    "  .mode(\"overwrite\")\n",
    "  .option(\"path\", \"/tmp/data/parquet/df_parquet\")\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf53985",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fs ls /tmp/data/parquet/df_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (spark\n",
    "       .read\n",
    "       .option(\"header\", \"true\")\n",
    "       .option(\"mode\", \"FAILFAST\")\t # exit if any errors\n",
    "       .option(\"nullValue\", \"\")\n",
    "       .schema(schema)\n",
    "       .csv(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f166f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97aae7",
   "metadata": {},
   "source": [
    "## ORC Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf94300",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"orc\")\n",
    "      .option(\"path\", orc_file)\n",
    "      .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8184e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3690ef",
   "metadata": {},
   "source": [
    "## Avro Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea4ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "      .format(\"avro\")\n",
    "      .option(\"path\", avro_file)\n",
    "      .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e791b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c31463a",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import image\n",
    "\n",
    "image_dir = \"../../databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "images_df = spark.read.format(\"image\").load(image_dir)\n",
    "images_df.printSchema()\n",
    "\n",
    "images_df.select(\"image.height\", \"image.width\", \"image.nChannels\", \"image.mode\", \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac969e",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5691bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../databricks-datasets/learning-spark-v2/cctvVideos/train_images/\"\n",
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "  .load(path))\n",
    "\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e331e",
   "metadata": {},
   "source": [
    "To ignore any partitioning data discovery in a directory, you can set the recursiveFileLookup to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9321bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_files_df = (spark.read.format(\"binaryFile\")\n",
    "   .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "   .option(\"recursiveFileLookup\", \"true\")\n",
    "   .load(path))\n",
    "binary_files_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18271114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

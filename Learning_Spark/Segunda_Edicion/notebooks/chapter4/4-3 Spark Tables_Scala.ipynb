{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717152b9",
   "metadata": {},
   "source": [
    "# Spark Tables\n",
    "\n",
    "This notebook shows how to use Spark Catalog Interface API to query databases, tables, and columns.\n",
    "\n",
    "A full list of documented methods is available [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8818940e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.11:4050\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1633075528522)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "us_flights_file: String = ../../databricks-datasets/learning-spark-v2/flights/departuredelays.csv\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val us_flights_file = \"../../databricks-datasets/learning-spark-v2/flights/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f179ac5e",
   "metadata": {},
   "source": [
    "## Create Managed Tables\n",
    "\n",
    "https://stackoverflow.com/questions/50914102/why-do-i-get-a-hive-support-is-required-to-create-hive-table-as-select-error/54552891"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e518ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Hive support is required to CREATE Hive TABLE (AS SELECT);",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Hive support is required to CREATE Hive TABLE (AS SELECT);",
      "'CreateTable `learn_spark_db`.`us_delay_flights_tbl`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, ErrorIfExists",
      "",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.$anonfun$apply$4(rules.scala:462)",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.$anonfun$apply$4$adapted(rules.scala:460)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:174)",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:460)",
      "  at org.apache.spark.sql.execution.datasources.HiveOnlyCheck$.apply(rules.scala:458)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$46(CheckAnalysis.scala:699)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$46$adapted(CheckAnalysis.scala:699)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:699)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)",
      "  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)",
      "  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "// Create database and managed tables\n",
    "spark.sql(\"DROP DATABASE IF EXISTS learn_spark_db CASCADE\") \n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "spark.sql(\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d7001",
   "metadata": {},
   "source": [
    "## Display the databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64f8147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Database] = [name: string, description: string ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2996d22",
   "metadata": {},
   "source": [
    "## Read our US Flights table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "441354c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = (spark.read.format(\"csv\")\n",
    "      .schema(\"date STRING, delay INT, distance INT, origin STRING, destination STRING\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .option(\"path\", \"departuredelays.csv\")\n",
    "      .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a4c2b",
   "metadata": {},
   "source": [
    "## Save into our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79103819",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"us_delay_flights_tbl_scala\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d6ad4",
   "metadata": {},
   "source": [
    "## Display tables within a Database\n",
    "\n",
    "Note that the table is MANGED by Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d029865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Column] = [name: string, description: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"us_delay_flights_tbl_scala\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be1f7c1",
   "metadata": {},
   "source": [
    "## Display Columns for a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e03725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Column] = [name: string, description: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"us_delay_flights_tbl_scala\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd7f271",
   "metadata": {},
   "source": [
    "## Create Unmanaged Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f596c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Drop the database and create unmanaged tables\n",
    "spark.sql(\"DROP DATABASE IF EXISTS learn_spark_db CASCADE\")\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "spark.sql(\"CREATE TABLE us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING) USING csv OPTIONS (path '/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a779a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

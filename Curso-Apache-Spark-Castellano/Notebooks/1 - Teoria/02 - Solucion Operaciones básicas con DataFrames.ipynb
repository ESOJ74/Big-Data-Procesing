{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Operaciones b\u00e1sicas con DataFrames"}, {"cell_type": "markdown", "metadata": {}, "source": "## Descripci\u00f3n de las variables"}, {"cell_type": "markdown", "metadata": {}, "source": "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> est\u00e1 compuesto por las siguientes variables referidas siempre al a\u00f1o 2018:\n\n1. **Month** 1-4\n2. **DayofMonth** 1-31\n3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n4. **FlightDate** fecha del vuelo\n5. **Origin** c\u00f3digo IATA del aeropuerto de origen\n6. **OriginCity** ciudad donde est\u00e1 el aeropuerto de origen\n7. **Dest** c\u00f3digo IATA del aeropuerto de destino\n8. **DestCity** ciudad donde est\u00e1 el aeropuerto de destino  \n9. **DepTime** hora real de salida (local, hhmm)\n10. **DepDelay** retraso a la salida, en minutos\n11. **ArrTime** hora real de llegada (local, hhmm)\n12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n13. **Cancelled** si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n14. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n15. **Diverted** si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n16. **ActualElapsedTime** tiempo real invertido en el vuelo\n17. **AirTime** en minutos\n18. **Distance** en millas\n19. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n20. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n22. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n23. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista."}, {"cell_type": "markdown", "metadata": {}, "source": "Leemos el fichero CSV utilizando el delimitador por defecto de Spark (\",\"). La primera l\u00ednea contiene encabezados (nombres de columnas) por lo que no es parte de los datos y debemos indicarlo con la opci\u00f3n header."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# Esto no hace nada: la lectura es lazy as\u00ed que no se lee en realidad hasta que ejecutemos una acci\u00f3n sobre flightsDF\n# Solamente se comprueba que exista el fichero en esa ruta, y se leen los nombres de columnas\nflightsDF = spark.read.option(\"header\", \"true\")\\\n                 .csv(\"gs://data/flights-jan-apr-2018.csv\")"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Month: string (nullable = true)\n |-- DayofMonth: string (nullable = true)\n |-- DayOfWeek: string (nullable = true)\n |-- FlightDate: string (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: string (nullable = true)\n |-- DepDelay: string (nullable = true)\n |-- ArrTime: string (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: string (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: string (nullable = true)\n |-- ActualElapsedTime: string (nullable = true)\n |-- AirTime: string (nullable = true)\n |-- Distance: string (nullable = true)\n |-- CarrierDelay: string (nullable = true)\n |-- WeatherDelay: string (nullable = true)\n |-- NASDelay: string (nullable = true)\n |-- SecurityDelay: string (nullable = true)\n |-- LateAircraftDelay: string (nullable = true)\n\n"}], "source": "# Veamos el esquema (nombre y tipo de dato de cada columna). Esto son solamente metadatos, por lo que no es ninguna acci\u00f3n.\nflightsDF.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Todas las columnas son cadenas de caracteres porque no hemos indicado el tipo de dato para cada columna ni tampoco le hemos pedido a Spark que intente inferirlo a partir de los datos. No queremos que todas sean string porque hay algunas num\u00e9ricas que deber\u00edan ser tratadas como tales. Vamos a intentar inferir el esquema. Esto supone una lectura un poco m\u00e1s lenta y tambi\u00e9n es m\u00e1s lento que una tercera opci\u00f3n que consiste en indicar expl\u00edcitamente el esquema para los datos en el momento de la lectura, que es la opci\u00f3n recomendada si sabemos de antemano qu\u00e9 tipo va a tener cada columna. Si lo hici\u00e9semos de esa manera, en caso de que no se pueda leer con ese esquema obtendr\u00edamos un error."}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n\n"}], "source": "from pyspark.sql import functions as F\n\nflightsDF = spark.read\\\n                 .option(\"header\", \"true\")\\\n                 .option(\"inferSchema\", \"true\")\\\n                 .csv(\"gs://ucmbucket/data/flights-jan-apr-2018.csv\") # pon aqu\u00ed la ruta en tu bucket\n\n# Ensuciamos a prop\u00f3sito la variable ArrDelay para que pase a ser un string como suele pasar con frecuencia\nflightsDF = flightsDF.withColumn(\"ArrDelay\",\\\n                                 F.when(F.rand(123) < 0.1, \"NA\").otherwise(F.col(\"ArrDelay\")))\n\nflightsDF.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Ahora tiene mejor pinta, aunque todav\u00eda hay algunas columnas cuyo tipo de dato sigue siendo string cuando la intuici\u00f3n nos dice que deber\u00edan ser enteros."}, {"cell_type": "markdown", "metadata": {}, "source": "## Operaciones b\u00e1sicas con Data Frames"}, {"cell_type": "markdown", "metadata": {}, "source": "### Consultamos las columnas que tiene el DF"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": "['Month',\n 'DayofMonth',\n 'DayOfWeek',\n 'FlightDate',\n 'Origin',\n 'OriginCity',\n 'Dest',\n 'DestCity',\n 'DepTime',\n 'DepDelay',\n 'ArrTime',\n 'ArrDelay',\n 'Cancelled',\n 'CancellationCode',\n 'Diverted',\n 'ActualElapsedTime',\n 'AirTime',\n 'Distance',\n 'CarrierDelay',\n 'WeatherDelay',\n 'NASDelay',\n 'SecurityDelay',\n 'LateAircraftDelay']"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.columns"}, {"cell_type": "markdown", "metadata": {}, "source": "### Contamos el n\u00famero de filas"}, {"cell_type": "markdown", "metadata": {}, "source": "Es una de las primeras cosas que nos preguntamos sobre un dataset: n\u00famero de filas y columnas (cu\u00e1ntos ejemplos y cu\u00e1ntas variables tenemos para describirlos). Puesto que vamos a llevar a cabo varias transformaciones al DataFrame `flightsDF` a partir de este punto, vamos a usar `cache`() para que Spark lo mantenga en memoria en lugar de liberar la memoria ocupada tras cada acci\u00f3n."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Los datos tienen 23 columnas\nLos datos tienen 2503113 filas\n"}], "source": "# Extraemos los nombres de columna. Esto son solo metadatos de DataFrame, y est\u00e1n en el driver. No es necesaria ninguna\n# operaci\u00f3n sobre el cluster para recuperar la variable interna columns de cualquier DataFrame\nprint(\"Los datos tienen {0} columnas\".format(len(flightsDF.columns)))\n\nflightsDF.cache()        # Esta l\u00ednea no hace c\u00e1lculos, pero Spark anota que debe mantener este DF en memoria tras la primera vez que sea materializado\nrows = flightsDF.count() # Esto es una acci\u00f3n que obligar\u00e1 a que flightsDF sea materializado. Para ello, habr\u00e1 que llevar a cabo\n                         # las transformaciones que lo generan en la celda anterior: read y withColumn, que est\u00e1n pendientes\n\nprint(\"Los datos tienen {0} filas\".format(rows))"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "2503113"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Seleccionar columnas por nombre"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------+-------+\n|Month|DayofMonth|ArrTime|\n+-----+----------+-------+\n|    1|        14|   null|\n|    1|         3|   1506|\n|    1|         6|   1543|\n|    1|         7|   1455|\n|    1|         8|   1509|\n|    1|         9|   1504|\n|    1|        10|   1455|\n|    1|        11|   1452|\n|    1|        12|   1748|\n|    1|        13|   1514|\n+-----+----------+-------+\nonly showing top 10 rows\n\n"}], "source": "resultDF = flightsDF.select(\"Month\", \"DayofMonth\", \"ArrTime\")\nresultDF.show(10) # los nombres son sensibles a may\u00fasculas"}, {"cell_type": "markdown", "metadata": {}, "source": "### Filtramos (retenemos) filas en base a los valores de una o varias columnas"}, {"cell_type": "markdown", "metadata": {}, "source": "La mayor\u00eda de las transformaciones est\u00e1n definidas en el paquete `pyspark.sql.functions`, por lo que es frecuente importar el paquete completo con un alias, como `F`, en lugar de importar cada funci\u00f3n individual. A partir de ese momento usamos `F.` antes del nombre de cada funci\u00f3n, para decirle a python d\u00f3nde buscar esa funci\u00f3n."}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "flightsJanuary20 = None\nflightsJanuary20 = flightsDF\\\n                      .where(\"DayofMonth = 20 and Month = 1\")\\\n                      .withColumn(\"DistKm\", 1.6 * flightsDF[\"Distance\"])\\\n                      .withColumn(\"DobleDistancia\", 2 * F.col(\"DistKm\"))\n#                      .select(\"Month\", \"ArrTime\", \"DistKm\") # encadenamos dos transformaciones: esto no desencadena ninguna operaci\u00f3n"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n |-- DistKm: double (nullable = true)\n |-- DobleDistancia: double (nullable = true)\n\n"}], "source": "flightsJanuary20.printSchema()"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Hubo 16176 vuelos el 20 de enero de 2018\n+-----+-------+\n|Month|ArrTime|\n+-----+-------+\n|    1|   1533|\n|    1|   1734|\n|    1|   2025|\n+-----+-------+\nonly showing top 3 rows\n\n"}], "source": "# La funci\u00f3n col se utiliza para decir que nos estamos refiriendo a la columna cuyo nombre se pasa como argumento\nflightsJanuary20 = flightsDF\\\n                      .where((F.col(\"DayofMonth\") == 20) & (F.col(\"Month\") == 1))\\\n                      .select(\"Month\", \"ArrTime\")\\\n                      .cache() # encadenamos dos transformaciones: esto no desencadena ninguna operaci\u00f3n\n\n# C\u00faantos vuelos hay el 20 de enero de 2018\n# La operaci\u00f3n count() es una acci\u00f3n, as\u00ed que obliga a materializar flightsJanuary20. Para ello es necesario ejecutar\n# las transformaciones where() y select() que pusimos en esta celda, aplicadas a flightsDF. De hecho, si no hubi\u00e9semos\n# cacheado flightsDF en las celdas anteriores, tambi\u00e9n habr\u00eda que materializarlo otra vez, y para eso se leer\u00eda de \n# nuevo el CSV desde HDFS\nrowsJanuary20 = flightsJanuary20.count()\n\nprint(\"Hubo {0} vuelos el 20 de enero de 2018\".format(rowsJanuary20))\n\n# Esto es otra acci\u00f3n aplicada sobre el DataFrame flightsJanuary. Como flightsJanuary NO ha sido cacheado,\n# entonces las operaciones \"where\" y \"select\" se necesitan ejecutar DE NUEVO para poder hacer el show()\nflightsJanuary20.show(3)"}, {"cell_type": "markdown", "metadata": {}, "source": "**No olvidemos cachear el DataFrame cuando tengamos previsto hacer varias operaciones sobre \u00e9l, o de lo contrario estaremos repitiendo muchas veces los c\u00e1lculos previos que llevaron a ese DataFrame!!**"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": "20519"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "# Tambi\u00e9n podemos indicar el filtrado como un string con un trozo de c\u00f3digo SQL\n# Recordemos que where y filter son exactamente equivalentes\nflightsJanuary31 = flightsDF.filter(\"DayofMonth = 31 and Month = 1\") # transformaci\u00f3n filter: Spark no ejecuta nada\n\nflightsJanuary31.count() # acci\u00f3n count: obliga a materializar flightsJanuary31 para lo cual se tiene que ejecutar filter"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<b>RECUERDA:</b> Spark hace todas estas operaciones de manera distribuida en el cluster, por tanto cada nodo del cluster est\u00e1 filtrando filas de entre aquellas que est\u00e1n presentes en ese nodo (m\u00e1s precisamente, en ese executor). Cada executor env\u00eda al driver el recuento de cu\u00e1ntas filas ha filtrado, y los resultados son agregados en el driver para mostrar el recuento total.\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: muestra por pantalla el retraso en la llegada, el aeropuerto de origen y de destino de aquellos vuelos que tuvieron lugar en Domingo y con un retraso a la llegada mayor de 15 minutos. Muestra el esquema de dicho DataFrame resultante.\n\n</div>"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+------+----+\n|ArrDelay|Origin|Dest|\n+--------+------+----+\n|    21.0|   BOS| BUF|\n|    16.0|   BOS| JFK|\n|    22.0|   LGA| MSN|\n|   136.0|   BOS| JFK|\n|   119.0|   DTW| JFK|\n|   687.0|   DTW| MBS|\n|    90.0|   CVG| DTW|\n|    61.0|   GNV| ATL|\n|    48.0|   ATL| SHV|\n|    45.0|   SHV| ATL|\n|   158.0|   MSP| EWR|\n|    17.0|   LAS| FLL|\n|   238.0|   MCO| BOS|\n|    61.0|   BOS| FLL|\n|    34.0|   SLC| JFK|\n|    80.0|   JFK| PWM|\n|    23.0|   MCO| EWR|\n|    19.0|   OAK| JFK|\n|    63.0|   SJU| BDL|\n|    50.0|   LAX| JFK|\n+--------+------+----+\nonly showing top 20 rows\n\n"}], "source": "flightsDomingoDF = flightsDF.select(\"ArrDelay\", \"Origin\", \"Dest\")\\\n                            .filter(\"DayOfWeek = 7 and ArrDelay > 15\")\nflightsDomingoDF.show()"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+--------+\n|Origin|Dest|ArrDelay|\n+------+----+--------+\n|   BOS| BUF|    21.0|\n|   BOS| JFK|    16.0|\n|   LGA| MSN|    22.0|\n|   BOS| JFK|   136.0|\n|   DTW| JFK|   119.0|\n|   DTW| MBS|   687.0|\n|   CVG| DTW|    90.0|\n|   GNV| ATL|    61.0|\n|   ATL| SHV|    48.0|\n|   SHV| ATL|    45.0|\n|   MSP| EWR|   158.0|\n|   LAS| FLL|    17.0|\n|   MCO| BOS|   238.0|\n|   BOS| FLL|    61.0|\n|   SLC| JFK|    34.0|\n|   JFK| PWM|    80.0|\n|   MCO| EWR|    23.0|\n|   OAK| JFK|    19.0|\n|   SJU| BDL|    63.0|\n|   LAX| JFK|    50.0|\n+------+----+--------+\nonly showing top 20 rows\n\n"}], "source": "# sintaxis SQL pura: where(\"DayofWeek = 7 and ArrDelay > 15\")\n# otra posibilidad: where( (flightsDF[\"DayofWeek\"] == 7) & (flightsDF[\"ArrDelay\"] > 15) )\ndomingoretraso = flightsDF.where((F.col(\"DayofWeek\") == 7) & (F.col(\"ArrDelay\") > 15))\\\n                          .select('Origin','Dest','ArrDelay')\n\ndomingoretraso.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Seleccionar filas \u00fanicas"}, {"cell_type": "markdown", "metadata": {}, "source": "Para hacerse mejor idea de c\u00f3mo es una variable categ\u00f3rica, es l\u00f3gico querer cu\u00e1ntos valores distintos existen en nuestro dataset. Si consideramos todas las columnas, ser\u00eda raro tener dos filas exactamente iguales en todos los valores, pero si seleccionamos solamente una o unas pocas columnas, podemos ver cu\u00e1ntas combinaciones distintas de valores de esas columnas se dan en el dataset.\n"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Hay 2483784 filas distintas\n"}], "source": "distinctFlights = flightsDF.distinct()  # distinct es una transformaci\u00f3n que devuelve el DF sin las filas repetidas\ndistinctFlightsCount = distinctFlights.count() # count es una acci\u00f3n y provoca que se ejecute la transformaci\u00f3n distinct\n\nprint(\"Hay {0} filas distintas\".format(distinctFlightsCount))"}, {"cell_type": "markdown", "metadata": {}, "source": "Si seleccionamos solo las columnas `Origin` y `Dest` y nos quedamos con las filas distintas (quitamos repetidos), entonces obtenemos un DataFrame con los posibles aeropuertos de origen y destino, es decir, aquellos trayectos que existen en un vuelo (pueden aparecer varias veces porque seguramente existir\u00e1n muchos vuelos a lo largo de 2018 entre un origen y un destino)"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n    <b>TU TURNO</b>: \u00bf<b>cu\u00e1ntas</b> combinaciones de Origin y Dest existen?\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "**Respuesta en la siguiente celda:**"}, {"cell_type": "code", "execution_count": 15, "metadata": {"scrolled": true}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Hay 5795 combinaciones de un aeropuerto de origen y uno de destino\n"}], "source": "countOriginDests = flightsDF.select(\"Origin\", \"Dest\").distinct().count()\n\nprint(\"Hay {0} combinaciones de un aeropuerto de origen y uno de destino\".format(countOriginDests))"}, {"cell_type": "markdown", "metadata": {}, "source": "Aunque tengamos 2.5 millones de filas, solo hay 5795 combinaciones distintas de un aeropuerto de origen y otro de destino.\n\n<div class=\"alert alert-block alert-success\">\n    <b>TU TURNO</b>: \u00bf<b>cu\u00e1ntos</b> aeropuertos de origen existen?\n</div>"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+\n|origDistintos|\n+-------------+\n|          356|\n+-------------+\n\nHay 356 aeropuertos desde los que puede partir un vuelo\n"}], "source": "distinctOrigins = flightsDF.select(\"Origin\")\\\n                           .distinct()\\\n                           .count()\n        \nflightsDF.select(F.countDistinct(\"Origin\").alias(\"origDistintos\")).show()\n\nprint(\"Hay {0} aeropuertos desde los que puede partir un vuelo\".format(distinctOrigins))"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: vamos a hacer esto para cada columna, en un bucle, para hacernos a la idea de cu\u00e1ntos valores hay. Esto solo tiene sentido en realidad para columnas categ\u00f3ricas y no para num\u00e9ricas donde casi todos los valores ser\u00e1n distintos.\n</div>"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------+---------+----------+------+----------+----+--------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n|Month|DayofMonth|DayOfWeek|FlightDate|Origin|OriginCity|Dest|DestCity|DepTime|DepDelay|ArrTime|ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|AirTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n+-----+----------+---------+----------+------+----------+----+--------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n|    4|        31|        7|       120|   356|       350| 356|     350|   1440|    1378|   1440|    1376|        2|               5|       2|              705|    669|    1466|        1094|         748|     609|          128|              739|\n+-----+----------+---------+----------+------+----------+----+--------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+\n\n"}], "source": "def dfConteos(sparkDF):\n    colsRecuento = [F.countDistinct(c).alias(c) for c in sparkDF.columns]\n    return sparkDF.select(colsRecuento)\n\nconteosDF = dfConteos(flightsDF)\nconteosDF.show()"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Existen 4 valores distintos en la columna Month\nExisten 31 valores distintos en la columna DayofMonth\nExisten 7 valores distintos en la columna DayOfWeek\nExisten 120 valores distintos en la columna FlightDate\nExisten 356 valores distintos en la columna Origin\nExisten 350 valores distintos en la columna OriginCity\nExisten 356 valores distintos en la columna Dest\nExisten 350 valores distintos en la columna DestCity\nExisten 1441 valores distintos en la columna DepTime\nExisten 1379 valores distintos en la columna DepDelay\nExisten 1441 valores distintos en la columna ArrTime\nExisten 1377 valores distintos en la columna ArrDelay\nExisten 2 valores distintos en la columna Cancelled\nExisten 6 valores distintos en la columna CancellationCode\nExisten 2 valores distintos en la columna Diverted\nExisten 706 valores distintos en la columna ActualElapsedTime\nExisten 670 valores distintos en la columna AirTime\nExisten 1466 valores distintos en la columna Distance\nExisten 1095 valores distintos en la columna CarrierDelay\nExisten 749 valores distintos en la columna WeatherDelay\nExisten 610 valores distintos en la columna NASDelay\nExisten 129 valores distintos en la columna SecurityDelay\nExisten 740 valores distintos en la columna LateAircraftDelay\n"}], "source": "for columnName in flightsDF.columns:\n\n    distinctValues = flightsDF.select(columnName).distinct().count()\n    \n    # No olvid\u00e9is indentar este comando para indicar que est\u00e1 dentro del cuerpo del bucle\n    print(\"Existen {0} valores distintos en la columna {1}\".format(distinctValues, columnName))\n"}, {"cell_type": "markdown", "metadata": {}, "source": "### Crear una nueva columna o reemplazar una existente por el resultado de operar con columnas existentes"}, {"cell_type": "markdown", "metadata": {}, "source": "Tenemos las distancias en millas. Vamos a convertirlas a km multiplicando las millas por 1.61. De nuevo, cada nodo del cluster har\u00e1 esta operaci\u00f3n localmente con los datos que se encuentran en ese nodo. El DataFrame resultante estar\u00e1 repartido en los mismos nodos. **No hay movimiento de datos ya que no se necesita ninguna informaci\u00f3n que no est\u00e9 presente en ese nodo** para efectuar la operaci\u00f3n."}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n |-- DistanceKm: double (nullable = true)\n\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\n|Month|DayofMonth|DayOfWeek|         FlightDate|Origin|  OriginCity|Dest|    DestCity|DepTime|DepDelay|ArrTime|ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|AirTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|        DistanceKm|\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\n|    1|        14|        7|2018-01-14 00:00:00|   SYR|Syracuse, NY| DTW| Detroit, MI|   null|    null|   null|    null|      1.0|               B|     0.0|             null|   null|   374.0|        null|        null|    null|         null|             null|            602.14|\n|    1|         3|        3|2018-01-03 00:00:00|   SYR|Syracuse, NY| LGA|New York, NY|   1348|   -10.0|   1506|   -13.0|      0.0|            null|     0.0|             78.0|   42.0|   198.0|        null|        null|    null|         null|             null|318.78000000000003|\n|    1|         6|        6|2018-01-06 00:00:00|   SYR|Syracuse, NY| LGA|New York, NY|   1410|    12.0|   1543|    24.0|      0.0|            null|     0.0|             93.0|   45.0|   198.0|        12.0|         0.0|    12.0|          0.0|              0.0|318.78000000000003|\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+------------------+\nonly showing top 3 rows\n\n"}], "source": "# withColumn is a transformation returing a new DataFrame with one extra column appended on the right\nflightsWithKm = flightsDF.withColumn(\"DistanceKm\", F.col(\"Distance\") * 1.61)\n\nflightsWithKm.printSchema()\n\nflightsWithKm.show(3)"}, {"cell_type": "markdown", "metadata": {}, "source": "El d\u00eda de la semana es una variable entera. Utilizar un entero o una variable categ\u00f3rica es una decisi\u00f3n que depende de qu\u00e9 tipo de modelo vayamos a ajustar. \u00bfTiene sentido considerar d\u00edas de la semana \"m\u00e1s grandes\" o \"m\u00e1s peque\u00f1os\", es decir, algo que se incrementa conforme \"se incrementa\" el d\u00eda de la semana de 1 a 7? Aqu\u00ed vamos a **reemplazar** la columna `DayOfWeek` que ya exist\u00eda, por una versi\u00f3n categ\u00f3rica como strings. Utilizamos `withColumn` pas\u00e1ndole como primer argumento el nombre de una columna que ya existe, lo cual indica que queremos reemplazarla, en el mismo lugar que ocupaba."}, {"cell_type": "code", "execution_count": 19, "metadata": {"scrolled": false}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: string (nullable = false)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n\n+---------+-------+-------+\n|DayOfWeek|DepTime|ArrTime|\n+---------+-------+-------+\n|   Sunday|   null|   null|\n|Wednesday|   1348|   1506|\n| Saturday|   1410|   1543|\n|   Sunday|   1347|   1455|\n|   Monday|   1350|   1509|\n|  Tuesday|   1351|   1504|\n|Wednesday|   1347|   1455|\n| Thursday|   1345|   1452|\n|   Friday|   1640|   1748|\n| Saturday|   1338|   1514|\n|   Monday|   1353|   1456|\n|  Tuesday|   1357|   1511|\n|Wednesday|   1526|   1622|\n| Thursday|   1354|   1509|\n|   Friday|   1344|   1449|\n| Saturday|   1433|   1533|\n|   Sunday|   1353|   1508|\n|   Monday|   1354|   1504|\n|  Tuesday|   1501|   1616|\n|Wednesday|   1355|   1515|\n+---------+-------+-------+\nonly showing top 20 rows\n\n"}], "source": "flightsCategoricalDay = flightsDF.withColumn(\"DayOfWeek\", F.when(F.col(\"DayOfWeek\") == 1, \"Monday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 2, \"Tuesday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 3, \"Wednesday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 4, \"Thursday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 5, \"Friday\")\\\n                                                           .when(F.col(\"DayOfWeek\") == 6, \"Saturday\")\\\n                                                           .otherwise(\"Sunday\"))\n\nflightsCategoricalDay.printSchema() # the column is still in the same position but has now string type\n\nflightsCategoricalDay.select(\"DayOfWeek\", \"DepTime\", \"ArrTime\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n    <b>CONSEJO:</b> El proceso de crear nuevas variables o <i>features</i> a partir de otras existentes, incluso incorporar variables de fuentes de datos externas (datos p\u00fablicos) y de limpiar o reemplazar variables tras normalizarla se denomina gen\u00e9ricamente <i>feature engineering</i> (ingenier\u00eda de variables). A veces tiene mucho que ver con conocimientos espec\u00edficos del dominio, aunque tambi\u00e9n con trucos estad\u00edsticos para normalizar siguiend m\u00e9todos bien conocidos.\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "La funci\u00f3n `when` es muy com\u00fan para re-categorizar una variable o para crear nuevas variables categ\u00f3ricas a partir de condiciones complejas acerca de lo que les ocurre a los valores de otras columnas en esa misma fila."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: crea una variable categ\u00f3rica de tipo string con dos categor\u00edas indicando si el d\u00eda de la semana es laborable o es fin de semana. Los valores deben ser \"laborable\" o \"finde\". Utiliza `withColumn` y `when`. Ser\u00e1 laborable cuando DayOfWeek est\u00e9 entre 1 y 5, y fin de semana cuando sea 6 o 7 (en resumen: \"en otro caso\" es fin de semana). \n</div>"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- Month: integer (nullable = true)\n |-- DayofMonth: integer (nullable = true)\n |-- DayOfWeek: integer (nullable = true)\n |-- FlightDate: timestamp (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: integer (nullable = true)\n |-- DepDelay: double (nullable = true)\n |-- ArrTime: integer (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: double (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: double (nullable = true)\n |-- ActualElapsedTime: double (nullable = true)\n |-- AirTime: double (nullable = true)\n |-- Distance: double (nullable = true)\n |-- CarrierDelay: double (nullable = true)\n |-- WeatherDelay: double (nullable = true)\n |-- NASDelay: double (nullable = true)\n |-- SecurityDelay: double (nullable = true)\n |-- LateAircraftDelay: double (nullable = true)\n |-- Laborable: string (nullable = false)\n\n+---------+---------+\n|DayOfWeek|Laborable|\n+---------+---------+\n|        7|    Finde|\n|        3|Laborable|\n|        6|    Finde|\n|        7|    Finde|\n|        1|Laborable|\n|        2|Laborable|\n|        3|Laborable|\n|        4|Laborable|\n|        5|Laborable|\n|        6|    Finde|\n|        1|Laborable|\n|        2|Laborable|\n|        3|Laborable|\n|        4|Laborable|\n|        5|Laborable|\n|        6|    Finde|\n|        7|    Finde|\n|        1|Laborable|\n|        2|Laborable|\n|        3|Laborable|\n+---------+---------+\nonly showing top 20 rows\n\n"}], "source": "flightsFindeLaborable = flightsDF.withColumn(\"Laborable\", F.when(F.col(\"DayOfWeek\")<= 5, \"Laborable\")\\\n                                                           .otherwise(\"Finde\"))\n\nflightsFindeLaborable.printSchema()\nflightsFindeLaborable.select(\"DayOfWeek\", \"Laborable\").show()"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+-----------+\n|Month|DayofMonth|DayOfWeek|         FlightDate|Origin|  OriginCity|Dest|    DestCity|DepTime|DepDelay|ArrTime|ArrDelay|Cancelled|CancellationCode|Diverted|ActualElapsedTime|AirTime|Distance|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|  TypeOfDay|\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+-----------+\n|    1|        14|   Sunday|2018-01-14 00:00:00|   SYR|Syracuse, NY| DTW| Detroit, MI|   null|    null|   null|    null|      1.0|               B|     0.0|             null|   null|   374.0|        null|        null|    null|         null|             null|    weekend|\n|    1|         3|Wednesday|2018-01-03 00:00:00|   SYR|Syracuse, NY| LGA|New York, NY|   1348|   -10.0|   1506|   -13.0|      0.0|            null|     0.0|             78.0|   42.0|   198.0|        null|        null|    null|         null|             null|working day|\n|    1|         6| Saturday|2018-01-06 00:00:00|   SYR|Syracuse, NY| LGA|New York, NY|   1410|    12.0|   1543|    24.0|      0.0|            null|     0.0|             93.0|   45.0|   198.0|        12.0|         0.0|    12.0|          0.0|              0.0|    weekend|\n+-----+----------+---------+-------------------+------+------------+----+------------+-------+--------+-------+--------+---------+----------------+--------+-----------------+-------+--------+------------+------------+--------+-------------+-----------------+-----------+\nonly showing top 3 rows\n\n"}], "source": "flightsCategoricalDay = flightsCategoricalDay.withColumn(\"TypeOfDay\", \n                            F.when(~(F.col(\"DayOfWeek\").isin({'Saturday','Sunday'})), \"laborable\")\\\n                             .otherwise(\"finde\"))\nflightsCategoricalDay.show(3) "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "findeDF = flightsDF.withColumn(\"finde\", F.when(F.col(\"DayOfWeek\") <= 5, \"laborable\")\\\n                                         .otherwise(\"finde\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Crear y seleccionar columnas al vuelo"}, {"cell_type": "markdown", "metadata": {}, "source": "`withColumn` no es la \u00fanica manera de crear columnas. Podemos usar `select` no solo para seleccionar columnas existentes sino tambi\u00e9n para crear nuevas columnas al vuelo y a la vez seleccionarlas. Vamos a seleccionar Origin y Dest, que ya existen, a la vez que seleccionamos una tercera columna que estamos creando al vuelo, con la transformaci\u00f3n de la distancia a km. Le ponemos como nombre \"DistanceKm\" en ese momento que la estamos creando, mediante la funci\u00f3n `alias`. Si no usamos `alias`, Spark le pondr\u00e1 a la nueva columna un nombre por defecto que en este caso ser\u00eda \"1.6 * Distance\". Se recomienda usar alias para dar nombre a las nuevas columnas."}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+----------+\n|Origin|Dest|DistanceKm|\n+------+----+----------+\n|   SYR| DTW|     598.4|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n+------+----+----------+\nonly showing top 20 rows\n\n"}], "source": "flightsDF.selectExpr(\"Origin\", \"Dest\", \"1.6*Distance AS DistanceKm\").show()"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+----------+\n|Origin|Dest|DistanceKm|\n+------+----+----------+\n|   SYR| DTW|     598.4|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n+------+----+----------+\nonly showing top 20 rows\n\n"}], "source": "flightsAirportsAndKm = flightsDF.select(\"Origin\",\\\n                                        \"Dest\",\\\n                                        (1.6 * F.col(\"Distance\")).alias(\"DistanceKm\"))\nflightsAirportsAndKm.show()"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+----+----------+\n|Origin|Dest|DistanceKm|\n+------+----+----------+\n|   SYR| DTW|     598.4|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n|   SYR| LGA|     316.8|\n+------+----+----------+\nonly showing top 5 rows\n\n"}], "source": "flightsAirportsAndKm = flightsDF.select(F.col(\"Origin\"),\\\n                                        F.col(\"Dest\"),\\\n                                        (1.6 * F.col(\"Distance\")).alias(\"DistanceKm\"))\n\nflightsAirportsAndKm.show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b>TU TURNO</b>: crear un DataFrame con tres columnas seleccionando \"Origin\", \"OriginCity\" y una nueva columna de tipo string creada concatenando Origin y OriginCity con un gui\u00f3n \"-\". Utilizar `withColumn` y dentro la funci\u00f3n `concat_ws` (concatenar con separador) con sintaxis <a target=\"_blank\" href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.concat_ws\">F.concat_ws(\"-\", columna1, columna2)</a> del paquete pyspark.sql.functions.\n</div>"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsConcat = flightsDF.withColumn(\"Concat\",F.concat_ws(\"-\",\"Origin\",\"OriginCity\"))\\\n                                     .select(\"Origin\",\"OriginCity\", \"Concat\")\nflightsConcat.show(5)"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+------------+----------------+\n|Origin|  OriginCity|     concatenado|\n+------+------------+----------------+\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n+------+------------+----------------+\nonly showing top 20 rows\n\n"}], "source": "concatenadoDF = flightsDF.withColumn(\"concatenado\", F.concat_ws(\"-\", F.col(\"Origin\"), F.col(\"OriginCity\")))\\\n                         .select(F.col(\"Origin\"), F.col(\"OriginCity\"), F.col(\"concatenado\"))\nconcatenadoDF.show()"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------+------------+----------------+\n|Origin|  OriginCity|     concatenado|\n+------+------------+----------------+\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n|   SYR|Syracuse, NY|SYR-Syracuse, NY|\n+------+------------+----------------+\nonly showing top 20 rows\n\n"}], "source": "concatenadoDF = flightsDF.select(F.col(\"Origin\"), F.col(\"OriginCity\"),\n                                 F.concat_ws(\"-\", F.col(\"Origin\"), F.col(\"OriginCity\")).alias(\"concatenado\"))\nconcatenadoDF.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Convertir el tipo de dato de una columna"}, {"cell_type": "markdown", "metadata": {}, "source": "Con frecuencia, Spark no infiere correctamente el tipo de cada columna. \n* A veces ocurre que una columna num\u00e9rica no se reconozca adecuadamente porque los datos tienen \"NA\" (un string) que quer\u00eda significar data faltante en el dataset original. \"NA\" no es un string especial para Spark, por lo que simplemente reconoce que la columna tiene tanto enteros como strings, y el tipo m\u00e1s general que infiere es string para esa columna. \n* Pero no es el caso aqu\u00ed porque no se da esto, y por eso hemos ensuciado al principio los datos a prop\u00f3sito. Vamos a limpiarlos y a convertir la columna `ArrDelay` a DoubleType como debe ser."}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "('Hay ', 249841, 'filas con NA en ArrDelay')\n"}], "source": "naCount = flightsDF.where(\"ArrDelay = 'NA'\").count()\n\n# Esto es sintaxis SQL, pero tambi\u00e9n podr\u00edamos haberla llamado como .where(F.col(\"ArrDelay\") == \"NA\"). Ambas son equivalentes.\n\nprint(\"Hay \", naCount, \"filas con NA en ArrDelay\")"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"data": {"text/plain": "2196457"}, "execution_count": 27, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.where(F.col(\"ArrDelay\") != \"NA\").count()"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"data": {"text/plain": "True"}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.is_cached"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": "#from pyspark.sql.types import DoubleType\nfrom pyspark.sql import types as T\n\nflightsDF = flightsDF.where((F.col(\"ArrDelay\") != \"NA\"))\\\n                     .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(T.DoubleType()))"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"data": {"text/plain": "False"}, "execution_count": 31, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.is_cached"}, {"cell_type": "markdown", "metadata": {}, "source": "### Ordenaci\u00f3n respecto a una columna"}, {"cell_type": "markdown", "metadata": {}, "source": "Es una transformaci\u00f3n que nos devuelve otro DataFrame ordenado seg\u00fan la(s) columna(s) indicada(s), sea de forma ascendente o descendente. El DataFrame original no se modifica (al igual que ocurre en cualquier transformaci\u00f3n). Se puede ordenar en base a una columna num\u00e9rica (lo m\u00e1s frecuente) o tambi\u00e9n categ\u00f3rica (los strings se ordenan alfab\u00e9ticamente). Para ordenar s\u00ed es necesario enviar informaci\u00f3n de unos nodos a otros puesto que no sabemos si existen o no datos mayores o menos que el valor que tenemos en el nodo (o m\u00e1s precisamente, en el worker)."}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+------+----+\n|ArrDelay|Origin|Dest|\n+--------+------+----+\n|  2475.0|   HNL| PPG|\n|  2454.0|   PPG| HNL|\n|  2023.0|   EGE| JFK|\n|  1778.0|   ORF| DFW|\n|  1757.0|   SMF| DFW|\n|  1717.0|   HNL| JFK|\n|  1685.0|   ORF| JFK|\n|  1650.0|   ABQ| DFW|\n|  1648.0|   SLC| DFW|\n|  1576.0|   IAH| MIA|\n+--------+------+----+\nonly showing top 10 rows\n\n"}], "source": "# Ordenamos los vuelos seg\u00fan ArrDelay\nsortedDF = flightsDF.orderBy(\"ArrDelay\")  # equivalente: flightsDF.orderBy(F.col(\"ArrDelay\"))\n\n# Orden ascendentemente por aeropuerto de origen (\"Origin\") y deshago los empates por arr_delay descendentemente\nsortedDF = flightsDF.orderBy(F.col(\"Origin\"), F.col(\"ArrDelay\").desc())  # equivalente: flightsDF.orderBy(F.col(\"ArrDelay\"))\n\nsortedDescDF = flightsDF.orderBy(\"ArrDelay\", ascending = False)\nsortedDescDF.select(\"ArrDelay\", \"Origin\", \"Dest\").show(10)"}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": "primeraFila = sortedDescDF.first()"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"data": {"text/plain": "1445"}, "execution_count": 37, "metadata": {}, "output_type": "execute_result"}], "source": "primeraFila.ArrTime"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"data": {"text/plain": "{'Month': 2,\n 'DayofMonth': 9,\n 'DayOfWeek': 5,\n 'FlightDate': datetime.datetime(2018, 2, 9, 0, 0),\n 'Origin': 'HNL',\n 'OriginCity': 'Honolulu, HI',\n 'Dest': 'PPG',\n 'DestCity': 'Pago Pago, TT',\n 'DepTime': 1002,\n 'DepDelay': 2482.0,\n 'ArrTime': 1445,\n 'ArrDelay': 2475.0,\n 'Cancelled': 0.0,\n 'CancellationCode': None,\n 'Diverted': 0.0,\n 'ActualElapsedTime': 343.0,\n 'AirTime': 305.0,\n 'Distance': 2599.0,\n 'CarrierDelay': 0.0,\n 'WeatherDelay': 2475.0,\n 'NASDelay': 0.0,\n 'SecurityDelay': 0.0,\n 'LateAircraftDelay': 0.0}"}, "execution_count": 35, "metadata": {}, "output_type": "execute_result"}], "source": "primeraFila.asDict()"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"data": {"text/plain": "[2599.0,\n 2599.0,\n 1746.0,\n 1212.0,\n 1431.0,\n 4983.0,\n 290.0,\n 569.0,\n 989.0,\n 964.0,\n 861.0,\n 1810.0,\n 282.0,\n 2504.0,\n 1235.0,\n 1363.0,\n 228.0,\n 1172.0,\n 861.0,\n 761.0]"}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": "listaRows = sortedDescDF.take(20)  # devuelve una lista de Python, de objetos Row\ndistancias = [r.Distance for r in listaRows]  # usamos sintaxis de listas por comprensi\u00f3n\ndistancias"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"data": {"text/plain": "71647"}, "execution_count": 40, "metadata": {}, "output_type": "execute_result"}], "source": "listaLAX = flightsDF.where(\"Origin = 'LAX'\").collect() # \u00a1\u00a1\u00a1\u00a1 CUIDADO !!!!\nlen(listaLAX)"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(Month=1, DayofMonth=1, DayOfWeek=1, FlightDate=datetime.datetime(2018, 1, 1, 0, 0), Origin='LAX', OriginCity='Los Angeles, CA', Dest='JFK', DestCity='New York, NY', DepTime=600, DepDelay=0.0, ArrTime=1414, ArrDelay=-2.0, Cancelled=0.0, CancellationCode=None, Diverted=0.0, ActualElapsedTime=314.0, AirTime=285.0, Distance=2475.0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None),\n Row(Month=1, DayofMonth=1, DayOfWeek=1, FlightDate=datetime.datetime(2018, 1, 1, 0, 0), Origin='LAX', OriginCity='Los Angeles, CA', Dest='FLL', DestCity='Fort Lauderdale, FL', DepTime=2120, DepDelay=10.0, ArrTime=452, ArrDelay=-5.0, Cancelled=0.0, CancellationCode=None, Diverted=0.0, ActualElapsedTime=272.0, AirTime=250.0, Distance=2343.0, CarrierDelay=None, WeatherDelay=None, NASDelay=None, SecurityDelay=None, LateAircraftDelay=None),\n Row(Month=1, DayofMonth=1, DayOfWeek=1, FlightDate=datetime.datetime(2018, 1, 1, 0, 0), Origin='LAX', OriginCity='Los Angeles, CA', Dest='MCO', DestCity='Orlando, FL', DepTime=2305, DepDelay=45.0, ArrTime=632, ArrDelay=27.0, Cancelled=0.0, CancellationCode=None, Diverted=0.0, ActualElapsedTime=267.0, AirTime=237.0, Distance=2218.0, CarrierDelay=2.0, WeatherDelay=0.0, NASDelay=0.0, SecurityDelay=0.0, LateAircraftDelay=25.0),\n Row(Month=1, DayofMonth=1, DayOfWeek=1, FlightDate=datetime.datetime(2018, 1, 1, 0, 0), Origin='LAX', OriginCity='Los Angeles, CA', Dest='JFK', DestCity='New York, NY', DepTime=851, DepDelay=21.0, ArrTime=1718, ArrDelay=29.0, Cancelled=0.0, CancellationCode=None, Diverted=0.0, ActualElapsedTime=327.0, AirTime=285.0, Distance=2475.0, CarrierDelay=0.0, WeatherDelay=0.0, NASDelay=29.0, SecurityDelay=0.0, LateAircraftDelay=0.0)]"}, "execution_count": 41, "metadata": {}, "output_type": "execute_result"}], "source": "listaLAX[0:4]"}, {"cell_type": "markdown", "metadata": {}, "source": "### Vamos a comprobar que al llevar al driver las 10 primeras filas, el DF estaba ordenado as\u00ed que esas filas est\u00e1n ordenadas"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": "lista = ordenadoDF.take(10) # esto devuelve una lista de python"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(ArrDelay=674.0, Origin=u'ABE', Dest=u'DTW'),\n Row(ArrDelay=581.0, Origin=u'ABE', Dest=u'DTW'),\n Row(ArrDelay=547.0, Origin=u'ABE', Dest=u'DTW'),\n Row(ArrDelay=505.0, Origin=u'ABE', Dest=u'DTW'),\n Row(ArrDelay=486.0, Origin=u'ABE', Dest=u'PGD'),\n Row(ArrDelay=459.0, Origin=u'ABE', Dest=u'ATL'),\n Row(ArrDelay=445.0, Origin=u'ABE', Dest=u'ORD'),\n Row(ArrDelay=394.0, Origin=u'ABE', Dest=u'PGD'),\n Row(ArrDelay=387.0, Origin=u'ABE', Dest=u'ORD'),\n Row(ArrDelay=385.0, Origin=u'ABE', Dest=u'PHL')]"}, "execution_count": 35, "metadata": {}, "output_type": "execute_result"}], "source": "lista"}, {"cell_type": "markdown", "metadata": {}, "source": "### Funciones de agregaci\u00f3n sobre el DataFrame completo"}, {"cell_type": "markdown", "metadata": {}, "source": "Spark tiene implementaciones distribuidas y paralelas de funciones de agregaci\u00f3n frecuentes como la media, min, max y desviaci\u00f3n t\u00edpica entre otras. Todas estas funciones reciben como argumento el objeto columna sobre el que la funci\u00f3n debe aplicarse. Lo m\u00e1s habitual es aplicarlas para agregar por grupos, pero esto lo veremos en el segundo notebook."}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a seleccionar vuelos con `ArrDelay` mayor de 15 minutos, y con ellos vamos a crear columnas con el min, max, media y desviaci\u00f3n t\u00edpica de la columna `ArrDelay`. Crearemos y seleccionaremos dichas columnas al vuelo, como vimos en la secci\u00f3n anterior."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsDF.printSchema()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "flightsDF.count()"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-----------+-----------+----------------+-----+-----------------+\n|     MeanArrDelay|MinArrDelay|MaxArrDelay|  StddevArrDelay|nOrig|      varArrDelay|\n+-----------------+-----------+-----------+----------------+-----+-----------------+\n|65.14031947595987|       16.0|     2475.0|82.9741567315577|  356|6884.710685313103|\n+-----------------+-----------+-----------+----------------+-----+-----------------+\n\n"}], "source": "# First we select those flights with at least 16 minutes of delay and then compute the aggregations\nagregacionesDF = flightsDF.where(F.col(\"ArrDelay\") > 15)\\\n                          .selectariance(\"ArrDelay\").alias(\"varArrDelay\")\n                            )\n\nagregacionesDF.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Existe una funci\u00f3n de Spark que hace casi todo esto por nosotros, llamada `summary`. Lo hace para cada columna num\u00e9rica que encuentre en el dataset. Es tambi\u00e9n una transformaci\u00f3n que devuelve un nuevo DataFrame con las m\u00e9tricas de resumen, sin modificar el DataFrame original."}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+------------------+------------------+-----------------+-------+------------+-------+------------+------------------+------------------+------------------+-----------------+---------+----------------+--------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+\n|summary|             Month|        DayofMonth|        DayOfWeek| Origin|  OriginCity|   Dest|    DestCity|           DepTime|          DepDelay|           ArrTime|         ArrDelay|Cancelled|CancellationCode|Diverted| ActualElapsedTime|           AirTime|         Distance|      CarrierDelay|      WeatherDelay|          NASDelay|      SecurityDelay| LateAircraftDelay|\n+-------+------------------+------------------+-----------------+-------+------------+-------+------------+------------------+------------------+------------------+-----------------+---------+----------------+--------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+\n|  count|           2196457|           2196457|          2196457|2196457|     2196457|2196457|     2196457|           2196457|           2194565|           2196456|          2196457|  2196457|               0| 2196457|           2196455|           2193799|          2196457|            392161|            392161|            392161|             392161|            392161|\n|   mean| 2.542153568223735|15.651195538997577|3.911423715556462|   null|        null|   null|        null|1334.5547798113053|  8.78106731858022|1474.1052882461565|3.245222647199558|      0.0|            null|     0.0|133.30549544607106|108.46920934871426|766.7883983160153|20.038349045417572| 3.426679348532873|14.440171256193247|0.08101519528968969|25.287509466775127|\n| stddev|1.1248902601679909|   8.7040446679045|2.004004849288884|   null|        null|   null|        null| 498.5787963773804|43.516549948136316| 527.5182396929978|45.76811341047794|      0.0|            null|     0.0| 72.02163083628703|  70.2327101184876|583.9983146546172|60.262440674861466|31.235080976693283| 32.58088835969429| 2.4474240335120734|50.049599667080315|\n|    min|                 1|                 1|                1|    ABE|Aberdeen, SD|    ABE|Aberdeen, SD|                 1|           -1280.0|                 1|          -1290.0|      0.0|            null|     0.0|           -1228.0|           -1244.0|             16.0|               0.0|               0.0|               0.0|                0.0|               0.0|\n|    25%|                 2|                 8|                2|   null|        null|   null|        null|               920|              -6.0|              1055|            -15.0|      0.0|            null|     0.0|              82.0|              58.0|            337.0|               0.0|               0.0|               0.0|                0.0|               0.0|\n|    50%|                 3|                16|                4|   null|        null|   null|        null|              1329|              -3.0|              1510|             -7.0|      0.0|            null|     0.0|             115.0|              90.0|            604.0|               1.0|               0.0|               2.0|                0.0|               2.0|\n|    75%|                 4|                23|                6|   null|        null|   null|        null|              1741|               6.0|              1914|              7.0|      0.0|            null|     0.0|             164.0|             138.0|           1009.0|              17.0|               0.0|              19.0|                0.0|              30.0|\n|    max|                 4|                31|                7|    YUM|    Yuma, AZ|    YUM|    Yuma, AZ|              2400|            2482.0|              2400|           2475.0|      0.0|            null|     0.0|             757.0|             696.0|           4983.0|            2007.0|            2475.0|            1515.0|              593.0|            2454.0|\n+-------+------------------+------------------+-----------------+-------+------------+-------+------------+------------------+------------------+------------------+-----------------+---------+----------------+--------+------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+\n\n"}], "source": "summariesDF = flightsDF.summary().cache()  # habr\u00eda sido buena idea hacer flightsDF.summary().cache()\nsummariesDF.show()"}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- summary: string (nullable = true)\n |-- Month: string (nullable = true)\n |-- DayofMonth: string (nullable = true)\n |-- DayOfWeek: string (nullable = true)\n |-- Origin: string (nullable = true)\n |-- OriginCity: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- DestCity: string (nullable = true)\n |-- DepTime: string (nullable = true)\n |-- DepDelay: string (nullable = true)\n |-- ArrTime: string (nullable = true)\n |-- ArrDelay: string (nullable = true)\n |-- Cancelled: string (nullable = true)\n |-- CancellationCode: string (nullable = true)\n |-- Diverted: string (nullable = true)\n |-- ActualElapsedTime: string (nullable = true)\n |-- AirTime: string (nullable = true)\n |-- Distance: string (nullable = true)\n |-- CarrierDelay: string (nullable = true)\n |-- WeatherDelay: string (nullable = true)\n |-- NASDelay: string (nullable = true)\n |-- SecurityDelay: string (nullable = true)\n |-- LateAircraftDelay: string (nullable = true)\n\n"}], "source": "summariesDF.printSchema()"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "8\n['summary', 'Month', 'DayofMonth', 'DayOfWeek', 'Origin', 'OriginCity', 'Dest', 'DestCity', 'DepTime', 'DepDelay', 'ArrTime', 'ArrDelay', 'Cancelled', 'CancellationCode', 'Diverted', 'ActualElapsedTime', 'AirTime', 'Distance', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay']\n"}], "source": "print(summariesDF.count()) # vamos a contar el n\u00famero de filas del DF resumen"}, {"cell_type": "markdown", "metadata": {}, "source": "### Conversi\u00f3n a Pandas"}, {"cell_type": "markdown", "metadata": {}, "source": "En general es complicado leer los resultados que muestra Spark. En ocasiones interesa convertirlos a un dataframe de Pandas, aunque esto implica traer todas las filas al driver, lo cual **debe hacerse con mucho cuidado y solo en casos en los que estemos seguros de que el DataFrame de Spark es peque\u00f1o y no desbordar\u00e1 la memoria del driver**."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<p><b>CONSEJO</b>: el concepto de dataframe como tabla con filas y columnas existe en muchos lenguajes de programaci\u00f3n (pandas de Python, dataframes de R, DataFrames de Spark). Sin embargo, Spark maneja DataFrames que est\u00e1n f\u00edsicamente distribuidos en las memorias RAM de las m\u00e1quinas del cluster, lo cual no tiene nada que ver con lo que hace la biblioteca Pandas o R que se ejecutan en una sola m\u00e1quina. Convertir un DataFrame de Spark en un dataframe de Pandas implica llevar todas las filas al driver, lo cual podr\u00eda resultar en una excepci\u00f3n Out-of-Memory si el contenido del DataFrame es m\u00e1s grande que la memoria RAM de la m\u00e1quina en la que se est\u00e1 ejecutando el programa driver. En este caso y en la mayor\u00eda de casos, se suele utilizar para mostrar res\u00famenes o agregados ya calculados previamente, y que sabemos que ocupan poco, con lo que no existe riesgo.</p>\n\n<p>Esta operaci\u00f3n tambi\u00e9n es muy frecuente cuando queremos representar gr\u00e1ficamente el contenido de un DataFrame de Spark. No existen funciones gr\u00e1ficas en Spark, por lo que tenemos que convertirlo en un dataframe de Pandas y utilizar las funciones gr\u00e1ficas de Python habituales (matplotlib, Seaborn o incluso la propia biblioteca Pandas) para mostrar lo que necesitemos.\n</p>\n</div>"}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [{"data": {"text/plain": "False"}, "execution_count": 69, "metadata": {}, "output_type": "execute_result"}], "source": "summariesDF.is_cached"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": "flightsPd = summariesDF.toPandas()"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>summary</th>\n      <th>Month</th>\n      <th>DayofMonth</th>\n      <th>DayOfWeek</th>\n      <th>Origin</th>\n      <th>OriginCity</th>\n      <th>Dest</th>\n      <th>DestCity</th>\n      <th>DepTime</th>\n      <th>DepDelay</th>\n      <th>...</th>\n      <th>CancellationCode</th>\n      <th>Diverted</th>\n      <th>ActualElapsedTime</th>\n      <th>AirTime</th>\n      <th>Distance</th>\n      <th>CarrierDelay</th>\n      <th>WeatherDelay</th>\n      <th>NASDelay</th>\n      <th>SecurityDelay</th>\n      <th>LateAircraftDelay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>2195756</td>\n      <td>2195756</td>\n      <td>2195756</td>\n      <td>2195756</td>\n      <td>2195756</td>\n      <td>2195756</td>\n      <td>2195756</td>\n      <td>2195756</td>\n      <td>2193859</td>\n      <td>...</td>\n      <td>0</td>\n      <td>2195756</td>\n      <td>2195754</td>\n      <td>2193103</td>\n      <td>2195756</td>\n      <td>391595</td>\n      <td>391595</td>\n      <td>391595</td>\n      <td>391595</td>\n      <td>391595</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>2.5428845463703618</td>\n      <td>15.647962706238763</td>\n      <td>3.9119838451995577</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1334.6682564000737</td>\n      <td>8.77650113339098</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>133.29546980217273</td>\n      <td>108.46217938692347</td>\n      <td>766.7551672408046</td>\n      <td>20.08061900688211</td>\n      <td>3.3975357193018296</td>\n      <td>14.406716122524546</td>\n      <td>0.08250360704299084</td>\n      <td>25.316081666006973</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stddev</td>\n      <td>1.1248089885230748</td>\n      <td>8.70562480253341</td>\n      <td>2.0043659206090956</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>498.6958329189281</td>\n      <td>43.464185589574306</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>71.9919447145365</td>\n      <td>70.20722828449857</td>\n      <td>583.9065781219789</td>\n      <td>60.56429305377853</td>\n      <td>30.855181839720377</td>\n      <td>32.37040004800845</td>\n      <td>2.526014171447696</td>\n      <td>49.95008971538814</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>ABE</td>\n      <td>Aberdeen, SD</td>\n      <td>ABE</td>\n      <td>Aberdeen, SD</td>\n      <td>1</td>\n      <td>-611.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>11.0</td>\n      <td>5.0</td>\n      <td>16.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>25%</td>\n      <td>2</td>\n      <td>8</td>\n      <td>2</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>920</td>\n      <td>-6.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>82.0</td>\n      <td>58.0</td>\n      <td>337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>50%</td>\n      <td>3</td>\n      <td>16</td>\n      <td>4</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1329</td>\n      <td>-3.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>115.0</td>\n      <td>90.0</td>\n      <td>604.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>75%</td>\n      <td>4</td>\n      <td>23</td>\n      <td>6</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>1741</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>164.0</td>\n      <td>138.0</td>\n      <td>1009.0</td>\n      <td>17.0</td>\n      <td>0.0</td>\n      <td>19.0</td>\n      <td>0.0</td>\n      <td>31.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>max</td>\n      <td>4</td>\n      <td>31</td>\n      <td>7</td>\n      <td>YUM</td>\n      <td>Yuma, AZ</td>\n      <td>YUM</td>\n      <td>Yuma, AZ</td>\n      <td>2400</td>\n      <td>2482.0</td>\n      <td>...</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>757.0</td>\n      <td>696.0</td>\n      <td>4983.0</td>\n      <td>2007.0</td>\n      <td>2475.0</td>\n      <td>1392.0</td>\n      <td>593.0</td>\n      <td>2454.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows \u00d7 23 columns</p>\n</div>", "text/plain": "  summary               Month          DayofMonth           DayOfWeek  \\\n0   count             2195756             2195756             2195756   \n1    mean  2.5428845463703618  15.647962706238763  3.9119838451995577   \n2  stddev  1.1248089885230748    8.70562480253341  2.0043659206090956   \n3     min                   1                   1                   1   \n4     25%                   2                   8                   2   \n5     50%                   3                  16                   4   \n6     75%                   4                  23                   6   \n7     max                   4                  31                   7   \n\n    Origin    OriginCity     Dest      DestCity             DepTime  \\\n0  2195756       2195756  2195756       2195756             2195756   \n1     None          None     None          None  1334.6682564000737   \n2     None          None     None          None   498.6958329189281   \n3      ABE  Aberdeen, SD      ABE  Aberdeen, SD                   1   \n4     None          None     None          None                 920   \n5     None          None     None          None                1329   \n6     None          None     None          None                1741   \n7      YUM      Yuma, AZ      YUM      Yuma, AZ                2400   \n\n             DepDelay         ...         CancellationCode Diverted  \\\n0             2193859         ...                        0  2195756   \n1    8.77650113339098         ...                     None      0.0   \n2  43.464185589574306         ...                     None      0.0   \n3              -611.0         ...                     None      0.0   \n4                -6.0         ...                     None      0.0   \n5                -3.0         ...                     None      0.0   \n6                 6.0         ...                     None      0.0   \n7              2482.0         ...                     None      0.0   \n\n    ActualElapsedTime             AirTime           Distance  \\\n0             2195754             2193103            2195756   \n1  133.29546980217273  108.46217938692347  766.7551672408046   \n2    71.9919447145365   70.20722828449857  583.9065781219789   \n3                11.0                 5.0               16.0   \n4                82.0                58.0              337.0   \n5               115.0                90.0              604.0   \n6               164.0               138.0             1009.0   \n7               757.0               696.0             4983.0   \n\n        CarrierDelay        WeatherDelay            NASDelay  \\\n0             391595              391595              391595   \n1  20.08061900688211  3.3975357193018296  14.406716122524546   \n2  60.56429305377853  30.855181839720377   32.37040004800845   \n3                0.0                 0.0                 0.0   \n4                0.0                 0.0                 0.0   \n5                1.0                 0.0                 2.0   \n6               17.0                 0.0                19.0   \n7             2007.0              2475.0              1392.0   \n\n         SecurityDelay   LateAircraftDelay  \n0               391595              391595  \n1  0.08250360704299084  25.316081666006973  \n2    2.526014171447696   49.95008971538814  \n3                  0.0                 0.0  \n4                  0.0                 0.0  \n5                  0.0                 2.0  \n6                  0.0                31.0  \n7                593.0              2454.0  \n\n[8 rows x 23 columns]"}, "execution_count": 67, "metadata": {}, "output_type": "execute_result"}], "source": "flightsPd"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": "otroDF = spark.createDataFrame(flightsPd)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.12"}}, "nbformat": 4, "nbformat_minor": 2}
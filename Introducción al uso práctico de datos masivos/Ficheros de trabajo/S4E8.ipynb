{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Construcción de Tipologías: Clústers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "#from pyspark.sql import SQLContext\n",
    "#sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bd5 = sqlContext.read.format(\n",
    "    \"com.databricks.spark.csv\"\n",
    ").option(\"header\", \"true\").load(\"file:/home/cloudera/Documents/Ficheros de trabajo/bd5.csv\", inferSchema=True)\n",
    "sqlContext.registerDataFrameAsTable(bd5, \"bd5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "a1  = VectorAssembler(\n",
    "    inputCols=['DepDelay','Distance','DayOfWeek',\n",
    "               'CRSDepTime','Horario','LogD'],\n",
    "    outputCol='features')\n",
    "\n",
    "bd6 = a1.transform(bd5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(bd6)\n",
    "bd6std = scalerModel.transform(bd6)\n",
    "\n",
    "pca2=PCA(k=2,inputCol='scaledFeatures',outputCol='pca_scaledfeatures')\n",
    "model2=pca2.fit(bd6std)\n",
    "bd6pca2=model2.transform(bd6std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clústers - K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "#4 clústers\n",
    "kmeans=KMeans(k=4,seed=123,featuresCol=\"pca_scaledfeatures\",maxIter=10, predictionCol=\"Cluster\")\n",
    "model3=kmeans.fit(bd6pca2)\n",
    "\n",
    "bd6Kmeans = model3.transform(bd6pca2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bd6Kmeans.select('pca_scaledfeatures','Cluster').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bd6Kmeans.groupBy('Cluster').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracterización de los Clústers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centers = model3.clusterCenters()\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(bd6Kmeans, \"bd6Kmeans\")\n",
    "\n",
    "g3 = sqlContext.sql(\"select Cluster, count(*) as n, \\\n",
    "               avg(DepDelay) as DepDelay, \\\n",
    "               avg(Distance) as Distance, \\\n",
    "               avg(DayOfWeek) as DayOfWeek, \\\n",
    "               avg(CRSDepTime) as CRSDepTime, \\\n",
    "               avg(Horario) as Horario, \\\n",
    "               avg(LogD) as LogD \\\n",
    "               from bd6Kmeans group by Cluster\")\n",
    "g3.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación Gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extraemos las componentes\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "p1=udf(lambda v:float(v[0]),FloatType())\n",
    "p2=udf(lambda v:float(v[1]),FloatType())\n",
    "\n",
    "bd6Kmeans=bd6Kmeans.withColumn('pca1',p1('pca_scaledfeatures')).withColumn('pca2',p2('pca_scaledfeatures'))\n",
    "\n",
    "pdf6 = bd6Kmeans.sample(False, 0.1, 0).select('pca1','pca2','Cluster').toPandas()\n",
    "pdf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "sns.lmplot(x=\"pca1\", y=\"pca2\", hue=\"Cluster\", fit_reg=False, data=pdf6)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
